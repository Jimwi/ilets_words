WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.814
[MUSIC PLAYING]

00:00:06.100 --> 00:00:08.830
SPEAKER 1: Daniel
Suarez came on the scene

00:00:08.830 --> 00:00:12.520
with his first novel,
"Daemon," like a Unix daemon,

00:00:12.520 --> 00:00:14.680
which he self-published
for a while

00:00:14.680 --> 00:00:18.040
and marketed on the
Internet to such success

00:00:18.040 --> 00:00:20.630
that actual mainstream
publishers eventually picked it

00:00:20.630 --> 00:00:21.130
up.

00:00:21.130 --> 00:00:23.707
But he did that back
before that was a thing.

00:00:23.707 --> 00:00:25.540
And since then, he's
become a New York Times

00:00:25.540 --> 00:00:30.360
best-selling author,
and I think that's it.

00:00:30.360 --> 00:00:31.420
That's Daniel.

00:00:31.420 --> 00:00:32.081
Come on.

00:00:32.081 --> 00:00:35.335
[APPLAUSE]

00:00:35.335 --> 00:00:37.210
DANIEL SUAREZ: It's so
great to see you guys.

00:00:37.210 --> 00:00:40.030
I'm also impressed how many
people came during lunch.

00:00:40.030 --> 00:00:42.040
Whenever I-- when I worked
in corporate America,

00:00:42.040 --> 00:00:45.010
I would try to schedule
meetings with lunch,

00:00:45.010 --> 00:00:46.480
but people can bring it in.

00:00:46.480 --> 00:00:48.670
That's good.

00:00:48.670 --> 00:00:51.250
So my whole shtick,
if you will, is

00:00:51.250 --> 00:00:54.400
that I write sci-fi
thrillers that are heavily

00:00:54.400 --> 00:00:59.290
based in real science, or real
software, real code and logic.

00:00:59.290 --> 00:01:00.997
And "Change Agent"
is no different.

00:01:00.997 --> 00:01:02.830
It's a bit of a departure
for me in that I'm

00:01:02.830 --> 00:01:04.840
writing about genetic editing.

00:01:04.840 --> 00:01:06.700
And it looks like my
timing might be good,

00:01:06.700 --> 00:01:11.020
because CRISPR is very
much in the news right now.

00:01:11.020 --> 00:01:13.880
Now, just so I can
take a survey here,

00:01:13.880 --> 00:01:15.970
how many people
here are relatively

00:01:15.970 --> 00:01:18.850
familiar with CRISPR
genetic editing?

00:01:18.850 --> 00:01:22.807
So that's-- OK, I'd
say about maybe 25%.

00:01:22.807 --> 00:01:24.640
And that's actually a
much higher percentage

00:01:24.640 --> 00:01:27.730
than if I was in a
random Barnes and Noble.

00:01:27.730 --> 00:01:29.860
This is a very technical crowd.

00:01:29.860 --> 00:01:31.540
So I'll just describe
it very quickly

00:01:31.540 --> 00:01:33.520
for those who
aren't as familiar.

00:01:33.520 --> 00:01:36.460
CRISPR is actually
an acronym that

00:01:36.460 --> 00:01:39.940
is not helpful in
determining what it's about--

00:01:39.940 --> 00:01:45.790
clustered regularly interspaced
short palindromic repeats.

00:01:45.790 --> 00:01:47.669
It's one of those
acronyms where you hear it

00:01:47.669 --> 00:01:49.460
and you say, OK, that
does not help at all.

00:01:49.460 --> 00:01:51.190
As a matter of fact,
I'm more confused.

00:01:51.190 --> 00:01:55.090
And essentially what
CRISPR is is back in 2012,

00:01:55.090 --> 00:01:59.020
some researchers found
out that bacteria has

00:01:59.020 --> 00:02:02.170
a way of modifying its own DNA.

00:02:02.170 --> 00:02:05.110
It's a part of a
bacterial immune system.

00:02:05.110 --> 00:02:08.650
What they noticed were there
were these repeating sections--

00:02:08.650 --> 00:02:11.740
and for those people who've
ever concatenated or parsed text

00:02:11.740 --> 00:02:14.980
files, it's essentially
a delimiter--

00:02:14.980 --> 00:02:17.380
and they noticed this
genetic delimiter.

00:02:17.380 --> 00:02:21.037
And within it were these
varying genetic sequences.

00:02:21.037 --> 00:02:22.495
And it turns out
that what that was

00:02:22.495 --> 00:02:26.650
was a rogue's gallery of
previous viral infections, sort

00:02:26.650 --> 00:02:28.310
of a profile.

00:02:28.310 --> 00:02:31.420
And so what would happen is
part of the bacterial cell

00:02:31.420 --> 00:02:33.580
would go in, retrieve
some of these,

00:02:33.580 --> 00:02:37.270
and then unzip the genetic
sequence of the DNA

00:02:37.270 --> 00:02:41.230
strand of this, the
bacteria, and scan looking

00:02:41.230 --> 00:02:43.940
for those nefarious characters.

00:02:43.940 --> 00:02:47.440
And if it found them, it would
cut the DNA, cut them out,

00:02:47.440 --> 00:02:50.480
and then the DNA would
reconnect itself.

00:02:50.480 --> 00:02:52.850
So what the researchers
did was they said,

00:02:52.850 --> 00:02:56.890
well, we see where
it has the target.

00:02:56.890 --> 00:02:59.994
Maybe we can program it
to replace after it cuts.

00:02:59.994 --> 00:03:02.410
And so what they essentially
did was they created a search

00:03:02.410 --> 00:03:05.560
and replace function
by leveraging

00:03:05.560 --> 00:03:10.540
this very ancient immunal
process of bacteria.

00:03:10.540 --> 00:03:11.751
And that became CRISPR.

00:03:11.751 --> 00:03:13.750
And that's what it means
by palindromic repeats.

00:03:13.750 --> 00:03:15.394
It's those repeating
segments where

00:03:15.394 --> 00:03:17.810
you can tell, OK, that's where
the rogue's gallery begins.

00:03:20.710 --> 00:03:23.380
Now, they've already
achieved amazing results

00:03:23.380 --> 00:03:25.030
in the lab using CRISPR.

00:03:25.030 --> 00:03:28.150
So if genetic editing seems
like it's far away, it's not.

00:03:28.150 --> 00:03:29.950
Just last month,
Chinese researchers

00:03:29.950 --> 00:03:32.570
were able to correct a genetic--

00:03:32.570 --> 00:03:37.780
a heritable genetic disorder
in a viable human embryo.

00:03:37.780 --> 00:03:40.090
Now, this embryo was
not brought to term,

00:03:40.090 --> 00:03:45.300
but it would have cured
this would-be child.

00:03:45.300 --> 00:03:47.050
And so of course
they're looking at a lot

00:03:47.050 --> 00:03:48.910
of other heritable
genetic disorders

00:03:48.910 --> 00:03:51.820
that they want to cure--

00:03:51.820 --> 00:03:56.200
cystic fibrosis, Huntington's
disease, hemophilia.

00:03:56.200 --> 00:03:59.830
These are diseases,
heritable genetic disorders

00:03:59.830 --> 00:04:03.550
that have only one or two
genetic coding problems that

00:04:03.550 --> 00:04:05.440
could be fixed by CRISPR.

00:04:05.440 --> 00:04:09.466
And this is what interested me
so much from a fictional point

00:04:09.466 --> 00:04:10.840
of view, because
I couldn't think

00:04:10.840 --> 00:04:15.910
of anything more important than
the ability to edit ourselves.

00:04:15.910 --> 00:04:18.640
And the fact that this
is not too far away--

00:04:18.640 --> 00:04:20.230
I think in the next
10 years or so, we

00:04:20.230 --> 00:04:23.770
are going to start seeing
these disorders being

00:04:23.770 --> 00:04:25.960
cured by genetic editing.

00:04:25.960 --> 00:04:27.640
And then the logical
next question

00:04:27.640 --> 00:04:30.551
is, what else could we do?

00:04:30.551 --> 00:04:32.800
And I think the first thing
that occurs to most people

00:04:32.800 --> 00:04:36.250
is, why not improve
yourself or your kids?

00:04:36.250 --> 00:04:38.860
Now, this is, right now,
not for living organisms.

00:04:38.860 --> 00:04:41.590
But if you have an
embryo, and let's say

00:04:41.590 --> 00:04:45.400
you wanted to give your child
30 more years of healthy living,

00:04:45.400 --> 00:04:48.880
you might change the
dapH-2 gene to give it

00:04:48.880 --> 00:04:50.740
a unique mutation
that is relatively

00:04:50.740 --> 00:04:53.530
rare in the natural world,
but if you can edit it,

00:04:53.530 --> 00:04:56.350
you could imbue your child
with 30 more healthy years

00:04:56.350 --> 00:04:57.210
of living.

00:04:57.210 --> 00:05:00.760
If you want them to have
fast-twitch muscle fibers,

00:05:00.760 --> 00:05:03.871
give them MEF2, a slight edit.

00:05:03.871 --> 00:05:06.370
Actually, that would have helped
me, because if I'm limping,

00:05:06.370 --> 00:05:08.560
it's because I hurt my
shoulder the other day.

00:05:08.560 --> 00:05:11.650
If I had MEF2, I don't think
I would have been hurt.

00:05:11.650 --> 00:05:14.050
If you want to improve
your child's memory,

00:05:14.050 --> 00:05:16.710
you could make an edit to DLG3.

00:05:16.710 --> 00:05:18.220
And there's a whole
host of them--

00:05:18.220 --> 00:05:23.560
PCSK9 could give
them healthier heart,

00:05:23.560 --> 00:05:26.840
stronger bones for different
edits, on and on and on.

00:05:26.840 --> 00:05:28.780
And you can see where
this is heading.

00:05:28.780 --> 00:05:31.120
Now, we're only 10 years
or so out from the ability

00:05:31.120 --> 00:05:31.840
to do that.

00:05:31.840 --> 00:05:33.820
Now, is it safe?

00:05:33.820 --> 00:05:34.420
Probably not.

00:05:34.420 --> 00:05:35.720
Probably not just yet.

00:05:35.720 --> 00:05:39.640
That's because we barely
understand the human genome.

00:05:39.640 --> 00:05:41.650
We know these genes
do this, but we

00:05:41.650 --> 00:05:43.850
don't know what else they do.

00:05:43.850 --> 00:05:45.070
And that's because--

00:05:45.070 --> 00:05:49.812
I like to think of the human
genome as a Word document

00:05:49.812 --> 00:05:52.270
that somebody has been working
on for hundreds of thousands

00:05:52.270 --> 00:05:54.910
of years, but they
never erase anything.

00:05:54.910 --> 00:05:56.980
They just cross stuff out.

00:05:56.980 --> 00:05:59.140
And that's because we have
in our genomic sequence

00:05:59.140 --> 00:06:02.380
genes that aren't expressed,
but they are, in some ways,

00:06:02.380 --> 00:06:06.000
primordial remnants
of who we used to be.

00:06:06.000 --> 00:06:08.699
And so when we make an
edit, we are not always

00:06:08.699 --> 00:06:10.990
aware of what other things
we might be changing further

00:06:10.990 --> 00:06:12.680
down the line.

00:06:12.680 --> 00:06:14.620
But in this book,
"Change Agent,"

00:06:14.620 --> 00:06:18.100
I wanted to explore the
idea of that impulse, that

00:06:18.100 --> 00:06:21.610
impulse to modify ourselves
and to improve opportunities

00:06:21.610 --> 00:06:22.600
for our kids.

00:06:22.600 --> 00:06:24.412
So I envisioned a world--

00:06:24.412 --> 00:06:26.620
this is the first book I've
written, by the way, that

00:06:26.620 --> 00:06:28.030
takes place in the future.

00:06:28.030 --> 00:06:30.190
Takes place in 2045.

00:06:30.190 --> 00:06:34.900
And by this time, I envisioned
that black-market embryo labs

00:06:34.900 --> 00:06:38.950
would be fairly common, and that
they would be offering perhaps

00:06:38.950 --> 00:06:41.770
risky edits to
expectant parents that

00:06:41.770 --> 00:06:45.010
want to give their children
a leg up in the world.

00:06:45.010 --> 00:06:46.990
And the end result
of this would be

00:06:46.990 --> 00:06:50.470
to rapidly accelerate
human evolution

00:06:50.470 --> 00:06:54.290
by an order of magnitude.

00:06:54.290 --> 00:06:57.460
Now, my protagonist in this
book is an Interpol agent

00:06:57.460 --> 00:07:01.207
who is tasked with trying to
locate these embryo clinics

00:07:01.207 --> 00:07:02.290
and try to shut them down.

00:07:02.290 --> 00:07:03.748
And again, it's
because of the risk

00:07:03.748 --> 00:07:07.210
they represent without
a full understanding

00:07:07.210 --> 00:07:10.870
of the human genome, if we start
making willy-nilly changes.

00:07:10.870 --> 00:07:14.020
The thing about CRISPR is,
these changes are passed down

00:07:14.020 --> 00:07:16.690
to all future
generations, so there's

00:07:16.690 --> 00:07:18.370
no easy way to erase it.

00:07:18.370 --> 00:07:20.350
Because if we start
changing things

00:07:20.350 --> 00:07:22.840
and we didn't understand the
effects of the first change

00:07:22.840 --> 00:07:25.450
we made, we start to stack
up all these random changes

00:07:25.450 --> 00:07:27.800
very quickly.

00:07:27.800 --> 00:07:29.740
And that's why he
tries to prevent it.

00:07:29.740 --> 00:07:32.440
I also foresee that this will
be an immensely profitable

00:07:32.440 --> 00:07:35.290
activity, embryo labs,
because think about it--

00:07:35.290 --> 00:07:39.160
if you can make your child
taller, have perfect vision,

00:07:39.160 --> 00:07:42.130
the temptation would be
so strong to do that.

00:07:42.130 --> 00:07:44.890
And the people who
would be willing to pay

00:07:44.890 --> 00:07:47.050
would find somebody
willing to do it.

00:07:47.050 --> 00:07:49.990
And that's primarily because
CRISPR is not a difficult thing

00:07:49.990 --> 00:07:50.930
to do.

00:07:50.930 --> 00:07:54.400
And I hesitate as I
say this, but you guys

00:07:54.400 --> 00:07:57.790
could get on Google, for
instance, and Google CRISPR,

00:07:57.790 --> 00:08:01.900
and for about $1,000, you
could set up your own home

00:08:01.900 --> 00:08:04.030
genetic editing lab.

00:08:04.030 --> 00:08:04.880
And that's no joke.

00:08:04.880 --> 00:08:05.590
You can.

00:08:05.590 --> 00:08:09.550
I was up at a TED summit
event this summer in Banff,

00:08:09.550 --> 00:08:13.060
and everybody got to edit
bacteria and imbue it

00:08:13.060 --> 00:08:15.220
with bioluminescence.

00:08:15.220 --> 00:08:16.750
And it didn't take
long to learn,

00:08:16.750 --> 00:08:18.880
and it didn't take
much equipment.

00:08:18.880 --> 00:08:21.550
Now, that is giving
rise to a renaissance

00:08:21.550 --> 00:08:23.410
in what's called
synthetic biology, that

00:08:23.410 --> 00:08:27.730
is creating synthetic organisms
that are built custom.

00:08:27.730 --> 00:08:30.790
And of course, CRISPR is
a tremendous tool in this.

00:08:30.790 --> 00:08:34.809
So we're building E.
coli bacteria and yeasts

00:08:34.809 --> 00:08:37.659
and algae that do useful work.

00:08:37.659 --> 00:08:39.760
And it's very much
like programming,

00:08:39.760 --> 00:08:42.309
because of course DNA is
very much like software.

00:08:42.309 --> 00:08:45.550
But we're programming
the hardware, in a way.

00:08:45.550 --> 00:08:48.120
And George Church at
MIT is really the father

00:08:48.120 --> 00:08:49.631
of synthetic biology.

00:08:49.631 --> 00:08:51.630
And this has gotten to
the point where now there

00:08:51.630 --> 00:08:53.910
is an annual
competition every year.

00:08:53.910 --> 00:08:58.080
It's called iGEM, where
undergraduate teams compete

00:08:58.080 --> 00:09:01.190
to design the coolest
synthetic life form.

00:09:01.190 --> 00:09:03.580
And that is a really
21st-century contest,

00:09:03.580 --> 00:09:04.724
if you ask me.

00:09:04.724 --> 00:09:05.640
It's pretty damn cool.

00:09:08.250 --> 00:09:11.220
Now, I think a lot
of people might

00:09:11.220 --> 00:09:14.520
have a problem with what
happens in this book, which

00:09:14.520 --> 00:09:16.980
is that one of these very
profitable genetic editing

00:09:16.980 --> 00:09:22.740
cartels injects my protagonist
with a serum that changes him

00:09:22.740 --> 00:09:24.330
as an adult--

00:09:24.330 --> 00:09:27.990
modifies his DNA.

00:09:27.990 --> 00:09:31.080
But two years ago,
researchers at the University

00:09:31.080 --> 00:09:33.150
of Washington, I
believe it was, they

00:09:33.150 --> 00:09:36.210
injected squirrel
monkeys in the eye

00:09:36.210 --> 00:09:39.390
and imbued them
with color vision.

00:09:39.390 --> 00:09:41.760
And this happened two years ago.

00:09:41.760 --> 00:09:45.060
So this was done
by pairing CRISPR

00:09:45.060 --> 00:09:48.690
with a virus to try to
propagate it through the cells.

00:09:48.690 --> 00:09:51.600
Now, researchers are
starting with the eye

00:09:51.600 --> 00:09:55.800
because eyes in mammals are
a relatively self-contained

00:09:55.800 --> 00:09:57.930
micro ecosystem, if you will.

00:09:57.930 --> 00:10:00.210
They don't want to have these
edits spread willy-nilly

00:10:00.210 --> 00:10:03.480
throughout the body, and the
eye is a fairly self-contained

00:10:03.480 --> 00:10:04.620
platform.

00:10:04.620 --> 00:10:07.560
So they've already had some
really interesting successes

00:10:07.560 --> 00:10:09.730
at that.

00:10:09.730 --> 00:10:11.612
I also like to tell people--

00:10:11.612 --> 00:10:13.320
and it's going to be
harder in this room,

00:10:13.320 --> 00:10:15.660
because sometimes my crowds
skew a little older--

00:10:15.660 --> 00:10:19.840
but thinking back 30
years, back in 1987,

00:10:19.840 --> 00:10:23.610
when I was much younger,
we basically would go

00:10:23.610 --> 00:10:25.380
to video stores and bookstores.

00:10:25.380 --> 00:10:28.920
We'd receive newspapers
and magazines at our home.

00:10:28.920 --> 00:10:32.460
We received or sought
physical goods.

00:10:32.460 --> 00:10:35.520
And of course now everyone
has a portable camera,

00:10:35.520 --> 00:10:38.100
computer phone that
allows them to broadcast

00:10:38.100 --> 00:10:39.705
to the entire world.

00:10:39.705 --> 00:10:41.580
And that has radically
transformed everything

00:10:41.580 --> 00:10:45.060
from business, economics,
our human relationships.

00:10:45.060 --> 00:10:47.950
And that's a dramatic
change in just 30 years.

00:10:47.950 --> 00:10:52.860
But I think the next
30 years, by 2045, '47,

00:10:52.860 --> 00:10:54.682
will make that
change look minor.

00:10:54.682 --> 00:10:56.640
I think there's going to
be a fourth Industrial

00:10:56.640 --> 00:10:59.430
Revolution of genetics
and synthetic biology

00:10:59.430 --> 00:11:02.430
that is going to radically
change the way we manufacture

00:11:02.430 --> 00:11:03.890
products.

00:11:03.890 --> 00:11:06.660
And of course, this, I
think, this fourth Industrial

00:11:06.660 --> 00:11:09.840
Revolution, will be the first
one where we as human beings

00:11:09.840 --> 00:11:10.740
physically change.

00:11:15.330 --> 00:11:17.250
Part of the thing
that fascinated me

00:11:17.250 --> 00:11:22.200
also about this book, this idea
of having a character change--

00:11:22.200 --> 00:11:24.630
it sounds familiar,
because it is.

00:11:24.630 --> 00:11:27.450
It's a recurring motif
throughout mythology.

00:11:27.450 --> 00:11:29.940
It's an identity switch.

00:11:29.940 --> 00:11:32.800
It's losing one's identity,
changing one's face.

00:11:32.800 --> 00:11:36.030
And you can go back to
Greek mythology of Zeus.

00:11:36.030 --> 00:11:39.240
You could go back to the
legend of King Arthur,

00:11:39.240 --> 00:11:41.100
or "The Prince and the Pauper."

00:11:41.100 --> 00:11:45.930
It's both a fear and
kind of an alluring idea

00:11:45.930 --> 00:11:47.146
to change yourself.

00:11:47.146 --> 00:11:49.020
And what I wanted to do
in "Change Agent" was

00:11:49.020 --> 00:11:54.030
take this most current research
and make that very realistic,

00:11:54.030 --> 00:11:57.240
to make it seem imminent
and relevant to us all.

00:12:03.100 --> 00:12:06.760
Now, a lot of the time people
glance at my work and they

00:12:06.760 --> 00:12:09.490
think that I'm
doomsaying, or somehow

00:12:09.490 --> 00:12:11.000
pessimistic about the future.

00:12:11.000 --> 00:12:14.690
I'm actually very
optimistic about the future.

00:12:14.690 --> 00:12:17.950
And when I look at
genetic editing,

00:12:17.950 --> 00:12:19.750
I again see
something like fire--

00:12:19.750 --> 00:12:22.540
a very useful tool that
could hurt us, but also

00:12:22.540 --> 00:12:24.730
offers us tremendous
capabilities.

00:12:24.730 --> 00:12:26.560
Again, think about
the ability to cure

00:12:26.560 --> 00:12:29.530
all of these heritable
genetic disorders.

00:12:29.530 --> 00:12:32.780
But more than that, set
us aside for a moment.

00:12:32.780 --> 00:12:34.870
There's also
cellular agriculture,

00:12:34.870 --> 00:12:36.580
and again, synthetic
biology creating

00:12:36.580 --> 00:12:39.970
new ways of manufacturing,
essentially biofacturing,

00:12:39.970 --> 00:12:44.680
programming E. coli
bacteria or algae or yeast

00:12:44.680 --> 00:12:47.260
to do things like
create biofuels,

00:12:47.260 --> 00:12:49.390
or to create pharmaceuticals.

00:12:49.390 --> 00:12:52.870
And most importantly, I think,
and what will cause the largest

00:12:52.870 --> 00:12:57.460
change, is cellular agriculture,
essentially redesigning

00:12:57.460 --> 00:13:01.660
these single-celled organisms
to create things like meat.

00:13:01.660 --> 00:13:04.930
Right now on Earth,
our ecosystem

00:13:04.930 --> 00:13:07.210
is being ravaged
by meat production.

00:13:07.210 --> 00:13:10.420
90% of the deforestation
happening in the Amazon

00:13:10.420 --> 00:13:14.150
is occurring to support
meat production.

00:13:14.150 --> 00:13:16.090
So we're leveling
all of these forests

00:13:16.090 --> 00:13:22.030
either to create pasture or
feed stocks for meat production.

00:13:22.030 --> 00:13:23.650
Now, if the human
population increases

00:13:23.650 --> 00:13:26.020
to 9 or 10 million people--

00:13:26.020 --> 00:13:29.590
billion people-- by 2050,
that will absolutely

00:13:29.590 --> 00:13:31.429
devastate the ecosystem.

00:13:31.429 --> 00:13:33.220
And of course, more
people are eating meat.

00:13:33.220 --> 00:13:36.010
Meat consumption is
going up drastically.

00:13:36.010 --> 00:13:38.830
With cellular
agriculture, companies

00:13:38.830 --> 00:13:41.345
can grow just the muscle
tissue, not the whole cow,

00:13:41.345 --> 00:13:44.320
or not the whole pig
or the whole chicken.

00:13:44.320 --> 00:13:46.630
And what this
essentially does is

00:13:46.630 --> 00:13:50.080
you're re-creating exactly
the DNA of the meat

00:13:50.080 --> 00:13:51.430
that you consume.

00:13:51.430 --> 00:13:54.010
And the great thing about
DNA is everyone can check it.

00:13:54.010 --> 00:13:56.197
It's the original open source.

00:13:56.197 --> 00:13:58.780
So you could take a look to see
that that steak is genetically

00:13:58.780 --> 00:14:00.300
identical.

00:14:00.300 --> 00:14:03.340
And they're not quite
there yet, but if we

00:14:03.340 --> 00:14:06.100
adopt cellular
agriculture to grow meat,

00:14:06.100 --> 00:14:10.390
we use 1% of the land and
only 10% of the water.

00:14:10.390 --> 00:14:12.370
So there is an ick factor.

00:14:12.370 --> 00:14:14.470
It freaks people out.

00:14:14.470 --> 00:14:19.270
But there's also the fact that
if we don't do it, or try it,

00:14:19.270 --> 00:14:23.230
we could completely
devastate the Earth.

00:14:23.230 --> 00:14:25.150
There's also other
changes to crops

00:14:25.150 --> 00:14:26.620
that we can make
with CRISPR, and I

00:14:26.620 --> 00:14:28.161
think the biggest
single change would

00:14:28.161 --> 00:14:32.890
be creating C4 photosynthesis
rice, which is already

00:14:32.890 --> 00:14:35.170
a project that's underway.

00:14:35.170 --> 00:14:37.940
Right now, rice uses
C3 photosynthesis,

00:14:37.940 --> 00:14:39.457
which is less efficient.

00:14:39.457 --> 00:14:41.290
Now, a third of the
population of the planet

00:14:41.290 --> 00:14:45.210
relies upon rice to survive.

00:14:45.210 --> 00:14:48.400
Corn and sugarcane
use C4 photosynthesis,

00:14:48.400 --> 00:14:50.040
which is more efficient.

00:14:50.040 --> 00:14:53.490
The effort is to change
the photosynthesis of rice,

00:14:53.490 --> 00:14:56.350
common strains of
rice, and in doing so,

00:14:56.350 --> 00:14:58.050
it would double the
production of rice

00:14:58.050 --> 00:15:00.870
and halve the amount
of water it needs.

00:15:00.870 --> 00:15:03.934
Otherwise the rice
would remain identical.

00:15:03.934 --> 00:15:05.850
And this is really where
we get into, I think,

00:15:05.850 --> 00:15:09.810
an interesting
area, which is GMOs.

00:15:09.810 --> 00:15:11.670
A lot of people
are against GMOs.

00:15:11.670 --> 00:15:13.830
I've historically
been against them,

00:15:13.830 --> 00:15:16.550
although for a different
reason than many other people.

00:15:16.550 --> 00:15:21.120
A lot of my objection
to GMOs is that they

00:15:21.120 --> 00:15:24.540
represent an entire
intellectual property virus--

00:15:24.540 --> 00:15:30.630
that is, a certain firm might
create a proprietary organism

00:15:30.630 --> 00:15:34.020
and then pair it
with a whole system,

00:15:34.020 --> 00:15:37.530
and then have that organism
spread its genomic sequence

00:15:37.530 --> 00:15:39.990
to other heirloom varieties
through natural pollination

00:15:39.990 --> 00:15:43.290
processes, and then basically
sue their competition out

00:15:43.290 --> 00:15:45.990
of existence.

00:15:45.990 --> 00:15:48.990
Now, that is one use of GMOs.

00:15:48.990 --> 00:15:51.480
But I think CRISPR is
going to radically change

00:15:51.480 --> 00:15:52.920
the GMO situation.

00:15:52.920 --> 00:15:56.700
I think GMOs are going to
go from being the purview

00:15:56.700 --> 00:16:01.440
of multimillion-dollar corporate
labs to being a tool that

00:16:01.440 --> 00:16:02.910
everyone will use--

00:16:02.910 --> 00:16:05.370
local communities,
tribes, even individuals.

00:16:05.370 --> 00:16:06.930
Because remember,
with CRISPR you

00:16:06.930 --> 00:16:10.500
could set up a genetic
editing lab with $1,000.

00:16:10.500 --> 00:16:13.320
Now, if we're living in a world
that is rapidly heating up,

00:16:13.320 --> 00:16:17.490
climate change is
causing fast changes that

00:16:17.490 --> 00:16:22.740
make pasture land or croplands
unviable for traditional crops,

00:16:22.740 --> 00:16:26.490
or if increased
storms and rising seas

00:16:26.490 --> 00:16:28.770
make it difficult to
keep using the crops

00:16:28.770 --> 00:16:32.460
that indigenous populations
have been using,

00:16:32.460 --> 00:16:35.510
they could conceivably
change them.

00:16:35.510 --> 00:16:37.200
Now, a good example--

00:16:37.200 --> 00:16:40.440
it goes back a few years, and
if I get this name wrong--

00:16:40.440 --> 00:16:43.740
I think it's Pamela
Ronald, her group at Davis,

00:16:43.740 --> 00:16:47.920
UCLA Davis, they
changed a strain of rice

00:16:47.920 --> 00:16:52.220
so that it could live
17 days under saltwater.

00:16:52.220 --> 00:16:54.830
Now, they took a
traditional Indian strain

00:16:54.830 --> 00:16:56.270
that had that ability.

00:16:56.270 --> 00:17:00.050
They changed a common strain of
rice that was very productive,

00:17:00.050 --> 00:17:02.450
and they imbued it
with that same ability.

00:17:02.450 --> 00:17:05.369
And then they gave it away.

00:17:05.369 --> 00:17:07.880
And I look at that, and
I find it very difficult

00:17:07.880 --> 00:17:10.619
to say that that's
a bad thing to do,

00:17:10.619 --> 00:17:12.990
when they're going to save
hundreds of thousands,

00:17:12.990 --> 00:17:15.720
if not millions
of people's lives.

00:17:15.720 --> 00:17:19.490
So GMOs, and the
anti-GMO movement,

00:17:19.490 --> 00:17:22.819
I think, needs to
evolve, because this

00:17:22.819 --> 00:17:24.530
is going to become a
very personal thing.

00:17:24.530 --> 00:17:26.030
People are going
to be able to start

00:17:26.030 --> 00:17:29.990
changing organisms in ways
that not only suit them,

00:17:29.990 --> 00:17:32.570
but that are imperative,
because they're a life and death

00:17:32.570 --> 00:17:33.290
matter.

00:17:33.290 --> 00:17:34.940
And it won't just
be just a decision

00:17:34.940 --> 00:17:36.410
made behind closed doors.

00:17:36.410 --> 00:17:38.676
It might be a
community decision.

00:17:38.676 --> 00:17:40.550
And a lot of this is
going to start happening

00:17:40.550 --> 00:17:42.260
very soon in coming years.

00:17:46.420 --> 00:17:48.550
Actually, I like to think of--

00:17:48.550 --> 00:17:51.640
you guys remember
this classic Apple ad

00:17:51.640 --> 00:17:54.940
where the PC was the
purview of, let's say, IBM.

00:17:54.940 --> 00:17:56.530
That's what it is symbolized as.

00:17:56.530 --> 00:17:59.290
And it's sort of represented as
this oppressive technological

00:17:59.290 --> 00:18:01.870
environment, and then the
runner comes up in full colors

00:18:01.870 --> 00:18:03.640
and shatters the screen.

00:18:03.640 --> 00:18:05.320
I think in many
ways, GMOs, are going

00:18:05.320 --> 00:18:07.390
to follow the same
pattern, that we're

00:18:07.390 --> 00:18:11.740
going to go from this very
controlled environment

00:18:11.740 --> 00:18:15.640
to rapid genetic change
that's open source,

00:18:15.640 --> 00:18:18.370
and people will be freely
exchanging information.

00:18:18.370 --> 00:18:22.600
So I think that's really
the next great renaissance.

00:18:22.600 --> 00:18:26.260
And of course, I cover all
of this in my new book,

00:18:26.260 --> 00:18:26.890
"Change Agent."

00:18:29.910 --> 00:18:33.790
Now, mastery of DNA, to me,
is the next great challenge

00:18:33.790 --> 00:18:35.120
for us.

00:18:35.120 --> 00:18:37.240
A lot of people talk
about AI, and of course AI

00:18:37.240 --> 00:18:38.690
is going to be important.

00:18:38.690 --> 00:18:40.420
But I think the
great fuse burning

00:18:40.420 --> 00:18:45.100
for us in developing mastery
of DNA is climate change.

00:18:45.100 --> 00:18:48.610
If we have methane salts
melting in the Arctic

00:18:48.610 --> 00:18:51.400
and possibly changing
our atmosphere,

00:18:51.400 --> 00:18:53.830
I think having the
ability to modify

00:18:53.830 --> 00:18:56.570
ourselves might come in handy.

00:18:56.570 --> 00:18:59.470
So again, this could
be both good and bad.

00:18:59.470 --> 00:19:03.380
And part of my incentive for
writing this book, as with most

00:19:03.380 --> 00:19:06.280
of my books, is I wanted
to popularize these ideas

00:19:06.280 --> 00:19:09.010
so that as many
people as possible,

00:19:09.010 --> 00:19:11.050
in the process of
being entertained,

00:19:11.050 --> 00:19:13.480
might also entertain
some of these ideas

00:19:13.480 --> 00:19:15.470
and think, wow,
that's interesting.

00:19:15.470 --> 00:19:17.230
Maybe they Google
it and they find out

00:19:17.230 --> 00:19:20.080
a lot of these things
are already underway.

00:19:20.080 --> 00:19:23.710
And if it does that, then that
makes me absolutely happy,

00:19:23.710 --> 00:19:27.920
and I'd say that I did my job.

00:19:27.920 --> 00:19:32.380
But first and foremost, I tried
to tell an exciting story.

00:19:32.380 --> 00:19:36.996
So that's my prepared talk.

00:19:36.996 --> 00:19:39.370
If you guys have any questions,
I'm happy to answer them.

00:19:39.370 --> 00:19:41.140
And please don't be shy.

00:19:41.140 --> 00:19:43.060
AUDIENCE: A typical question--

00:19:43.060 --> 00:19:45.480
who are your
influences in writing?

00:19:45.480 --> 00:19:49.380
What authors influenced you
in developing your stories?

00:19:49.380 --> 00:19:50.500
DANIEL SUAREZ: OK.

00:19:50.500 --> 00:19:53.840
Well, Neal Stephenson
was probably a later one,

00:19:53.840 --> 00:19:58.030
but we go back to Isaac
Asimov, Andre Norton--

00:19:58.030 --> 00:19:59.560
a lot of sci-fi.

00:19:59.560 --> 00:20:02.300
Loved sci-fi-- JRR
Tolkien, of course.

00:20:02.300 --> 00:20:05.196
And also John Dos Passos.

00:20:05.196 --> 00:20:07.570
I have an English literature
degree, so I've read widely.

00:20:07.570 --> 00:20:09.660
That's why I'm sitting
here trying to think,

00:20:09.660 --> 00:20:11.410
whose estate am I going
to piss off by not

00:20:11.410 --> 00:20:12.493
mentioning various people?

00:20:12.493 --> 00:20:17.800
But I think probably what
inspired me most to write-- oh,

00:20:17.800 --> 00:20:20.290
Carl Sagan would be one that
I would say would be a huge

00:20:20.290 --> 00:20:21.640
influence to me--

00:20:21.640 --> 00:20:24.272
As a matter of fact, probably
the most important influence.

00:20:24.272 --> 00:20:26.230
One of my favorite books,
aside from "Cosmos"--

00:20:26.230 --> 00:20:26.939
when I was a kid.

00:20:26.939 --> 00:20:29.229
I remember asking for Christmas,
like, I want "Cosmos,"

00:20:29.229 --> 00:20:31.900
and my parents were looking
like, that looks really boring.

00:20:31.900 --> 00:20:33.370
It's thick.

00:20:33.370 --> 00:20:34.210
And I loved it.

00:20:34.210 --> 00:20:36.730
But also "Demon-Haunted
World" by Carl Sagan,

00:20:36.730 --> 00:20:39.100
which is a super
relevant book now.

00:20:39.100 --> 00:20:40.390
I see you nodding your head.

00:20:40.390 --> 00:20:41.420
And it really is.

00:20:41.420 --> 00:20:42.700
And you think about that--

00:20:42.700 --> 00:20:45.220
his job, again--
he set upon himself

00:20:45.220 --> 00:20:47.920
the task to popularize
science, to make people

00:20:47.920 --> 00:20:50.950
understand what the big picture,
what the big issues were.

00:20:50.950 --> 00:20:55.270
And I humor myself
in thinking that I'm

00:20:55.270 --> 00:20:57.010
trying to do that
in some way here,

00:20:57.010 --> 00:20:59.470
for either cyber
or other issues.

00:20:59.470 --> 00:21:03.190
My big focus of my
work is I try to focus

00:21:03.190 --> 00:21:05.770
on technological
change, where that's

00:21:05.770 --> 00:21:07.660
coming, where it might go.

00:21:07.660 --> 00:21:09.760
So a lot of authors
who dealt with that--

00:21:09.760 --> 00:21:12.970
and I would say Carl
Sagan would be a huge one,

00:21:12.970 --> 00:21:15.580
but oh, man, I
could go on and on.

00:21:15.580 --> 00:21:17.980
Of course William
Gibson, obviously.

00:21:17.980 --> 00:21:21.720
But I'll stop it there,
because I'll just keep going.

00:21:21.720 --> 00:21:23.550
Yes.

00:21:23.550 --> 00:21:26.322
AUDIENCE: So you mentioned
that genetic engineering can

00:21:26.322 --> 00:21:27.780
be something that's
going to become

00:21:27.780 --> 00:21:29.670
more community-based
as opposed to something

00:21:29.670 --> 00:21:30.649
big corporations do.

00:21:30.649 --> 00:21:32.190
DANIEL SUAREZ: Yeah,
I believe it is.

00:21:32.190 --> 00:21:34.110
AUDIENCE: As we move
forward towards this,

00:21:34.110 --> 00:21:36.810
what can we do to guide people
to make sure that they do this

00:21:36.810 --> 00:21:39.180
in an ethical fashion, as
opposed to just starting

00:21:39.180 --> 00:21:40.382
to kill each other?

00:21:40.382 --> 00:21:41.340
DANIEL SUAREZ: Yes, OK.

00:21:41.340 --> 00:21:42.215
So the question was--

00:21:42.215 --> 00:21:44.756
this is probably going to come
through fine, because you have

00:21:44.756 --> 00:21:46.020
a microphone, but I'll recap.

00:21:46.020 --> 00:21:47.940
The question was,
how can we pursue

00:21:47.940 --> 00:21:51.000
this would-be renaissance in a
way that doesn't annihilate us

00:21:51.000 --> 00:21:53.640
or our neighbors
or everybody else?

00:21:53.640 --> 00:21:57.360
And I would say it is going to
take exactly this-- a dialogue,

00:21:57.360 --> 00:22:00.630
people knowing a bit
about it, seeing the news,

00:22:00.630 --> 00:22:02.190
and then talking about it.

00:22:02.190 --> 00:22:04.380
Because I've noticed
this pattern,

00:22:04.380 --> 00:22:06.960
and it's really well
covered in books like,

00:22:06.960 --> 00:22:09.690
I believe it's called "The
Master Switch" by Tim Wu.

00:22:09.690 --> 00:22:14.010
This long process of ingesting
new technologies for societies

00:22:14.010 --> 00:22:16.440
typically takes about 30 years.

00:22:16.440 --> 00:22:20.190
And that process of a radical
new technology coming in

00:22:20.190 --> 00:22:21.660
is really fractious.

00:22:21.660 --> 00:22:22.720
People sue each other.

00:22:22.720 --> 00:22:24.060
They fight each other.

00:22:24.060 --> 00:22:27.990
And I think I would provide
as an example aviation.

00:22:27.990 --> 00:22:30.450
Aviation, when it
first came through,

00:22:30.450 --> 00:22:34.740
invented a new legal term,
which was aerial trespass.

00:22:34.740 --> 00:22:36.240
And if you think
about, it's funny,

00:22:36.240 --> 00:22:37.906
because we just think
about it now like,

00:22:37.906 --> 00:22:39.247
well, people worked it out.

00:22:39.247 --> 00:22:40.830
But what happened
when airplanes first

00:22:40.830 --> 00:22:44.130
started flying low and slow over
people's land, very noisily,

00:22:44.130 --> 00:22:45.930
is people worried about privacy.

00:22:45.930 --> 00:22:48.330
They also didn't like people
flying over their land.

00:22:48.330 --> 00:22:51.840
And more particularly,
railroads, which carried mail,

00:22:51.840 --> 00:22:54.267
didn't like airmail
stealing their business.

00:22:54.267 --> 00:22:56.100
Now, if you look at the
shape of a railroad,

00:22:56.100 --> 00:22:59.160
it tends to be like a
wall, and so they really

00:22:59.160 --> 00:23:01.620
encouraged this idea
of aerial trespass,

00:23:01.620 --> 00:23:04.320
that if you cross our rail
lines, you're trespassing.

00:23:04.320 --> 00:23:06.669
And so this started
a series of lawsuits.

00:23:06.669 --> 00:23:08.460
And these were quickly
followed by lawsuits

00:23:08.460 --> 00:23:11.760
about privacy, lawsuits
about safety, product design,

00:23:11.760 --> 00:23:13.310
and on and on.

00:23:13.310 --> 00:23:15.960
The end result, after
almost 30 years,

00:23:15.960 --> 00:23:18.030
was codifying this
in regulations

00:23:18.030 --> 00:23:19.710
from all of those
legal precedents,

00:23:19.710 --> 00:23:21.380
and that became the FAA.

00:23:21.380 --> 00:23:23.880
To which I would say to people
that no matter what you think

00:23:23.880 --> 00:23:25.800
of the FAA, there's
very few people who

00:23:25.800 --> 00:23:29.076
would get on an aircraft that
was not certified by the FAA.

00:23:29.076 --> 00:23:33.780
So that process, fractious
as it is, it is useful.

00:23:33.780 --> 00:23:35.610
And I think we're
going to see this here.

00:23:35.610 --> 00:23:38.260
It's if we don't
see those arguments,

00:23:38.260 --> 00:23:40.830
if we don't see that vigorous
public debate, that's

00:23:40.830 --> 00:23:41.737
when I worry.

00:23:41.737 --> 00:23:43.320
Because as long as
people are directly

00:23:43.320 --> 00:23:45.630
addressing each other on this,
and really educating themselves

00:23:45.630 --> 00:23:46.130
on it--

00:23:46.130 --> 00:23:47.713
because I think this
is actually going

00:23:47.713 --> 00:23:50.370
to be the central issue of
the next 20 or 30 years--

00:23:50.370 --> 00:23:51.881
genetic editing.

00:23:51.881 --> 00:23:53.880
I mean, I can't think of
anything more impactful

00:23:53.880 --> 00:23:56.680
than the ability to
rapidly evolve ourselves.

00:23:56.680 --> 00:23:58.530
That would be
important in everything

00:23:58.530 --> 00:24:01.500
from space exploration to
dealing with climate change

00:24:01.500 --> 00:24:04.720
to dealing with pandemics,
on and on and on.

00:24:04.720 --> 00:24:07.417
And then, of course, you
have the eugenics questions.

00:24:07.417 --> 00:24:09.000
To what degree do
we change ourselves?

00:24:09.000 --> 00:24:11.110
What's acceptable?

00:24:11.110 --> 00:24:12.930
To me, it's a
fascinating question.

00:24:12.930 --> 00:24:15.780
But transparency-- that's
what's going to stop us

00:24:15.780 --> 00:24:17.130
from killing ourselves.

00:24:17.130 --> 00:24:20.790
Now, at this moment I'll
also bring up gene drives.

00:24:20.790 --> 00:24:24.780
Now, how many people here
have heard of gene drives?

00:24:24.780 --> 00:24:26.650
Got one person--
no, two people--

00:24:26.650 --> 00:24:27.270
three.

00:24:27.270 --> 00:24:28.320
Or you're just
pointing at your nose.

00:24:28.320 --> 00:24:29.070
OK, never mind.

00:24:29.070 --> 00:24:32.800
Don't go to an auction
is what I'd say.

00:24:32.800 --> 00:24:36.240
A gene drive is an
aspect of genetic editing

00:24:36.240 --> 00:24:38.370
that could turn into
a weapon very quickly.

00:24:38.370 --> 00:24:41.677
The Pentagon is certainly
concerned about gene drives.

00:24:41.677 --> 00:24:43.260
If you've heard of
gene drives, you've

00:24:43.260 --> 00:24:45.540
probably heard
about it in relation

00:24:45.540 --> 00:24:50.057
to Zika virus or malaria,
some mosquito-borne virus.

00:24:50.057 --> 00:24:52.140
And right now-- this is
not a few years from now--

00:24:52.140 --> 00:24:54.780
right now as we sit
here, scientists

00:24:54.780 --> 00:24:58.590
are creating a gene
drive that could crash

00:24:58.590 --> 00:25:00.840
certain species of mosquitoes.

00:25:00.840 --> 00:25:03.750
And the way they
do this is because

00:25:03.750 --> 00:25:06.540
through sexual
reproduction, the offspring

00:25:06.540 --> 00:25:10.230
acquire approximately half of
the genes from each parent.

00:25:10.230 --> 00:25:13.290
What a gene drive does
is that researchers

00:25:13.290 --> 00:25:17.430
use CRISPR editing, or maybe
some future type of editing,

00:25:17.430 --> 00:25:22.030
to make both those sides of the
gene exchange the same thing.

00:25:22.030 --> 00:25:24.270
It doesn't even have
to be from each parent.

00:25:24.270 --> 00:25:27.630
Basically, they guarantee that
a certain trait is passed down

00:25:27.630 --> 00:25:29.520
to the next generation.

00:25:29.520 --> 00:25:31.290
And if you keep
doing this, you could

00:25:31.290 --> 00:25:35.880
put in a fatal flaw, or a
way to somehow compromise

00:25:35.880 --> 00:25:36.570
that species.

00:25:36.570 --> 00:25:40.080
Or you can also make it so that
they don't transmit a virus.

00:25:40.080 --> 00:25:43.120
But either way, what are
the unintended consequences

00:25:43.120 --> 00:25:43.620
of that?

00:25:43.620 --> 00:25:46.140
If we, let's say, crash
a mosquito population,

00:25:46.140 --> 00:25:47.970
what other parts
of the ecosystem

00:25:47.970 --> 00:25:50.370
is it supporting that
we're not thinking about?

00:25:50.370 --> 00:25:52.830
Now, remember, this
can happen now.

00:25:52.830 --> 00:25:55.830
And also the part that the
Pentagon is concerned about,

00:25:55.830 --> 00:25:59.080
and other defense
agencies around the world,

00:25:59.080 --> 00:26:01.560
what about angry groups,
terrorist groups,

00:26:01.560 --> 00:26:03.690
creating a gene
drive that implants

00:26:03.690 --> 00:26:06.450
in the human genome
undesirable traits that

00:26:06.450 --> 00:26:10.950
later cause a specific
weakness or a fatal flaw?

00:26:10.950 --> 00:26:13.670
And again, remember,
$1,000 for a CRISPR lab.

00:26:13.670 --> 00:26:16.170
So I think it's going to be a
very proximate conversation we

00:26:16.170 --> 00:26:18.360
need to start having right away.

00:26:18.360 --> 00:26:20.430
But that would just
be one example of why

00:26:20.430 --> 00:26:22.810
that transparency and
that open communication

00:26:22.810 --> 00:26:25.480
is going to be pivotal,
because otherwise, I think,

00:26:25.480 --> 00:26:27.220
we do have some
things to worry about.

00:26:29.910 --> 00:26:32.340
AUDIENCE: So you seem
very well informed.

00:26:32.340 --> 00:26:34.600
I'm very excited
to read this book.

00:26:34.600 --> 00:26:36.780
And I was just curious,
how much of your time

00:26:36.780 --> 00:26:41.820
did you spend researching, and
where did you learn about this?

00:26:41.820 --> 00:26:43.920
DANIEL SUAREZ: I've found
a pattern with my books.

00:26:43.920 --> 00:26:45.630
Typically what happens
is-- by the way,

00:26:45.630 --> 00:26:48.087
in case it didn't come through
in audio, the question was,

00:26:48.087 --> 00:26:49.920
how much time do I spend
researching a book,

00:26:49.920 --> 00:26:52.450
and where do I learn
these types of things?

00:26:52.450 --> 00:26:55.695
Oh, I will start by saying
Google is really handy.

00:26:55.695 --> 00:26:57.000
[LAUGHTER]

00:26:57.000 --> 00:27:00.040
And this is not entirely
an answer to your question,

00:27:00.040 --> 00:27:02.820
which I'll continue to, but
Google Earth is probably

00:27:02.820 --> 00:27:05.570
the single best writers
research tool ever,

00:27:05.570 --> 00:27:08.070
because it has saved me a hell
of a lot of money in airfare,

00:27:08.070 --> 00:27:11.130
because I'll go to a location,
say, no, that's not right.

00:27:11.130 --> 00:27:13.200
I could even do Street
Views in someplace

00:27:13.200 --> 00:27:15.650
in Bangladesh-- very useful.

00:27:15.650 --> 00:27:18.600
Now, I spend
approximately a third

00:27:18.600 --> 00:27:22.530
of the time required to
write a book doing research.

00:27:22.530 --> 00:27:25.680
For me-- and I'm sure this is
different for every writer,

00:27:25.680 --> 00:27:27.390
but for me, it's
basically a third

00:27:27.390 --> 00:27:30.150
of my time conceiving
of the story

00:27:30.150 --> 00:27:31.800
and its structure
and its characters,

00:27:31.800 --> 00:27:34.140
a third of the time
doing research,

00:27:34.140 --> 00:27:36.910
and a third of the time
actually writing the book.

00:27:36.910 --> 00:27:38.910
And you can tell I've
worked in software before,

00:27:38.910 --> 00:27:42.600
because I organize
everything very carefully.

00:27:42.600 --> 00:27:46.950
I don't say-- it's not prose
reuse, but it's efficient.

00:27:46.950 --> 00:27:49.950
And that means that
by the time I'm

00:27:49.950 --> 00:27:51.990
ready to set pen to
paper to start writing,

00:27:51.990 --> 00:27:54.360
I typically don't
backtrack a lot.

00:27:54.360 --> 00:27:57.900
And so what I'll do is I
will have some idea that

00:27:57.900 --> 00:28:00.720
has taken hold of me,
typically, and genetic editing

00:28:00.720 --> 00:28:01.950
is a good example.

00:28:01.950 --> 00:28:04.020
Lethal autonomy for my
book "Kill Decision"

00:28:04.020 --> 00:28:06.330
is another example.

00:28:06.330 --> 00:28:08.400
Whatever the idea is
that I have for a book,

00:28:08.400 --> 00:28:10.200
it's typically one
that I'm imagining

00:28:10.200 --> 00:28:13.050
whenever I close my eyes
when I go to sleep at night.

00:28:13.050 --> 00:28:14.880
And if it won't
let me go, I know

00:28:14.880 --> 00:28:17.290
that it's an idea I have
to write a book about.

00:28:17.290 --> 00:28:19.020
And then it will
start thinking about,

00:28:19.020 --> 00:28:21.700
what is the big issue here
that I'm most concerned about?

00:28:21.700 --> 00:28:23.790
And that's really where
I'll root my research.

00:28:23.790 --> 00:28:25.800
And it will spread from there.

00:28:25.800 --> 00:28:27.990
Typically during
the research process

00:28:27.990 --> 00:28:30.350
I will discover so
many new cool things

00:28:30.350 --> 00:28:31.909
that I'll go in
different directions.

00:28:31.909 --> 00:28:33.450
But to me, it's very
important that I

00:28:33.450 --> 00:28:35.670
stop doing that by
the time I write,

00:28:35.670 --> 00:28:39.289
because I try to make these
several threads, a, b,

00:28:39.289 --> 00:28:40.830
and c thread of the
story, and I like

00:28:40.830 --> 00:28:42.720
to have them interweave
at the right moments.

00:28:42.720 --> 00:28:44.230
And if you change
things willy-nilly,

00:28:44.230 --> 00:28:45.438
it doesn't work out too well.

00:28:45.438 --> 00:28:47.250
So I try to do all my
significant changes

00:28:47.250 --> 00:28:51.800
during the research and
the structure component.

00:28:51.800 --> 00:28:53.300
Oh, and lots of
reading, by the way.

00:28:53.300 --> 00:28:54.457
I read lots of books.

00:28:54.457 --> 00:28:56.790
That's why at the end of every
single book I've written,

00:28:56.790 --> 00:28:58.740
there is a reading list.

00:28:58.740 --> 00:29:01.250
And if you just
flip through it, I

00:29:01.250 --> 00:29:03.180
guarantee you'll find
some interesting books,

00:29:03.180 --> 00:29:04.770
because they're
typically the books that

00:29:04.770 --> 00:29:06.394
were the most pivotally
important to me

00:29:06.394 --> 00:29:08.810
in the writing of the book.

00:29:08.810 --> 00:29:10.179
Yes.

00:29:10.179 --> 00:29:12.470
AUDIENCE: So the reason that
CRISPR-Cas9 is in the news

00:29:12.470 --> 00:29:14.360
right now is because
of a huge legal fight

00:29:14.360 --> 00:29:15.920
over its patent rights.

00:29:15.920 --> 00:29:19.100
So what leads you to believe
that this IBM world is going

00:29:19.100 --> 00:29:21.650
to get smashed as
opposed to continuing

00:29:21.650 --> 00:29:23.660
on kind of as it has been?

00:29:23.660 --> 00:29:26.390
DANIEL SUAREZ: Oh,
yeah, that lawsuit.

00:29:26.390 --> 00:29:28.910
See, I look at it in this way.

00:29:28.910 --> 00:29:30.740
There is the lawsuit
over who invented

00:29:30.740 --> 00:29:33.250
CRISPR, which is, I believe,
what you're referring to.

00:29:33.250 --> 00:29:36.020
Now, I think they settled
one portion of it already--

00:29:36.020 --> 00:29:38.900
which, to me-- now, of course
there's several other issues.

00:29:38.900 --> 00:29:40.520
It is so valuable,
I think that there

00:29:40.520 --> 00:29:42.800
is a lot of external
pressures saying,

00:29:42.800 --> 00:29:45.620
look, nobody is going to make
serious investments until we

00:29:45.620 --> 00:29:48.077
get this little matter of
who invented it solved,

00:29:48.077 --> 00:29:50.660
and I think there's going to be
a lot of pressure to solve it.

00:29:50.660 --> 00:29:52.868
And a lot of money will be
offered to various parties

00:29:52.868 --> 00:29:53.820
to settle.

00:29:53.820 --> 00:29:55.890
I'm going to boldly
make that prediction.

00:29:55.890 --> 00:29:57.920
I don't think it's
that bold, actually.

00:29:57.920 --> 00:30:00.890
But there's also the
simple matter that CRISPR

00:30:00.890 --> 00:30:02.810
and how it works
is generally known.

00:30:02.810 --> 00:30:05.062
And like I said, you
can get a hold of tools.

00:30:05.062 --> 00:30:06.770
If you're not running
some registered lab

00:30:06.770 --> 00:30:10.320
where you're selling a product,
that kind of changes things.

00:30:10.320 --> 00:30:12.290
If you're not selling,
let's say, software,

00:30:12.290 --> 00:30:14.720
and you're writing
code, people can

00:30:14.720 --> 00:30:17.240
take all sorts of liberties
in where that code came from,

00:30:17.240 --> 00:30:19.610
because it's very difficult
to say who's doing what.

00:30:19.610 --> 00:30:21.200
And whether that's
right or wrong,

00:30:21.200 --> 00:30:23.199
I think the impetus will
be that a lot of people

00:30:23.199 --> 00:30:25.310
will start messing around
with this technology,

00:30:25.310 --> 00:30:27.090
because it's so simple to use.

00:30:27.090 --> 00:30:28.730
And I think the
way it will start

00:30:28.730 --> 00:30:32.390
is in designing E. coli
bacteria or yeast or algae

00:30:32.390 --> 00:30:34.227
to do cool things.

00:30:34.227 --> 00:30:36.810
And I that's really where people
are going to cut their teeth.

00:30:36.810 --> 00:30:39.000
So regardless of what
legal case is going on,

00:30:39.000 --> 00:30:40.666
I think that's what's
going to drive it.

00:30:40.666 --> 00:30:44.090
And familiarity with the
tools will make people come up

00:30:44.090 --> 00:30:45.140
with a killer app.

00:30:45.140 --> 00:30:47.761
Now, whether that killer app
is literal or figurative,

00:30:47.761 --> 00:30:49.010
I think it remains to be seen.

00:30:52.020 --> 00:30:53.045
Hey, I know you.

00:30:53.045 --> 00:30:55.270
AUDIENCE: Yeah.

00:30:55.270 --> 00:30:57.150
[INAUDIBLE] Pasadena.

00:30:57.150 --> 00:31:02.140
So speaking of places in the US,
your book is not one of them.

00:31:02.140 --> 00:31:05.400
The locations are in
other parts of the world,

00:31:05.400 --> 00:31:06.780
in Singapore and Myanmar.

00:31:06.780 --> 00:31:09.480
And I want you to talk a little
bit about why you selected

00:31:09.480 --> 00:31:10.750
these parts of the world.

00:31:10.750 --> 00:31:12.870
DANIEL SUAREZ: So this
book, "Change Agent,"

00:31:12.870 --> 00:31:14.360
does not take place in America.

00:31:14.360 --> 00:31:17.010
And this is not the only one
of my books that doesn't.

00:31:17.010 --> 00:31:20.340
But there is a
particular passage

00:31:20.340 --> 00:31:23.785
in this book that has some
people concerned, I'll say,

00:31:23.785 --> 00:31:25.410
or at least they've
mentioned it to me,

00:31:25.410 --> 00:31:29.358
and that is that by 2045,
Silicon Valley is dead.

00:31:29.358 --> 00:31:31.550
[LAUGHTER]

00:31:31.550 --> 00:31:35.640
It's like a lot of e-mails like,
what is that supposed to mean?

00:31:35.640 --> 00:31:38.100
What it is an admonition.

00:31:38.100 --> 00:31:43.980
It is an admonition to America
to engage with reason and logic

00:31:43.980 --> 00:31:45.400
and science.

00:31:45.400 --> 00:31:48.600
And if we don't, the
rest of the world will.

00:31:48.600 --> 00:31:50.130
But it's not an
option, that we can

00:31:50.130 --> 00:31:55.570
fall back and lose our place as
the cutting edge of technology.

00:31:55.570 --> 00:31:57.840
And that's really
why it's there.

00:31:57.840 --> 00:32:01.770
I think that a great many parts
of the world that are dealing

00:32:01.770 --> 00:32:03.270
with climate change right now--

00:32:03.270 --> 00:32:05.190
no, it's not a hoax.

00:32:05.190 --> 00:32:08.750
And there's a drive
to question science

00:32:08.750 --> 00:32:12.030
and to muddy the waters here
that I find really worrisome,

00:32:12.030 --> 00:32:15.000
and so I wanted to perhaps
model what the world would

00:32:15.000 --> 00:32:17.400
look like if that continued.

00:32:17.400 --> 00:32:20.820
And I think anybody who is an
American reading this would

00:32:20.820 --> 00:32:24.000
probably realize that
this is quite possible,

00:32:24.000 --> 00:32:25.440
this world of 2045.

00:32:25.440 --> 00:32:28.200
Now, does that mean that all
Americans do not use science?

00:32:28.200 --> 00:32:28.950
No.

00:32:28.950 --> 00:32:30.783
It does mean that the
best and the brightest

00:32:30.783 --> 00:32:33.810
might go to Singapore
or to other countries

00:32:33.810 --> 00:32:36.100
where their skills
are used very well

00:32:36.100 --> 00:32:37.440
and they have a good future.

00:32:37.440 --> 00:32:40.500
And it would be great if we
could have that future here.

00:32:40.500 --> 00:32:44.820
But just so you know, Singapore
I chose because back in, I

00:32:44.820 --> 00:32:46.560
think it was 2006--

00:32:46.560 --> 00:32:48.660
I might have that
wrong by a year or so--

00:32:48.660 --> 00:32:50.880
they spent a billion
dollars creating something

00:32:50.880 --> 00:32:54.460
called Biopolis, which
is a biotech hub.

00:32:54.460 --> 00:32:54.960
It's a lab.

00:32:54.960 --> 00:32:56.790
It's a big lab complex,
and they wanted

00:32:56.790 --> 00:33:00.990
to lure the best geneticists
and biotech talent

00:33:00.990 --> 00:33:02.790
around the world
to come and work

00:33:02.790 --> 00:33:06.060
`unfettered` in these
beautiful, modern labs,

00:33:06.060 --> 00:33:08.490
to create an
infrastructure for what

00:33:08.490 --> 00:33:12.120
they felt was going to be
the next big tech revolution.

00:33:12.120 --> 00:33:13.790
And I tend to agree with them.

00:33:13.790 --> 00:33:17.760
Now, that's a serious investment
to make for that tiny nation.

00:33:17.760 --> 00:33:19.346
So that's why I chose Singapore.

00:33:23.160 --> 00:33:30.810
AUDIENCE: You talked
about [INAUDIBLE]

00:33:30.810 --> 00:33:40.880
ourselves from unethical or
morally [INAUDIBLE] behavior.

00:33:40.880 --> 00:33:50.560
How do you see us combating
incompetence or stupidity

00:33:50.560 --> 00:33:55.970
in this if anybody can
access these [INAUDIBLE]

00:33:55.970 --> 00:33:59.490
and anybody can augment E. coli.

00:33:59.490 --> 00:34:04.710
It's not uncommon for a
chemistry lab in a high school

00:34:04.710 --> 00:34:11.570
to blow up, or alcohol
bootleg to produce methanol.

00:34:11.570 --> 00:34:20.120
Well, if this is now an E. coli
with unintended gene drive,

00:34:20.120 --> 00:34:23.929
some hilarities might ensue.

00:34:23.929 --> 00:34:25.320
So what--

00:34:25.320 --> 00:34:27.840
DANIEL SUAREZ: What
controls are in place?

00:34:27.840 --> 00:34:29.340
I believe that's
what you're asking.

00:34:29.340 --> 00:34:31.679
Essentially this is--
an E. coli bacteria

00:34:31.679 --> 00:34:34.509
is one of the most common
organisms there is.

00:34:34.509 --> 00:34:36.300
As a matter of fact,
you can get on the web

00:34:36.300 --> 00:34:39.900
and from various
sites, you can purchase

00:34:39.900 --> 00:34:42.726
a stripped-down
version of E. coli.

00:34:42.726 --> 00:34:44.850
And again, I may have it
wrong by one or two genes,

00:34:44.850 --> 00:34:47.280
but I think it's 268
genes that they've

00:34:47.280 --> 00:34:49.139
reduced E. coli bacteria down.

00:34:49.139 --> 00:34:51.389
And all it does, all
this organism does,

00:34:51.389 --> 00:34:53.170
is maintain itself
and reproduce itself.

00:34:53.170 --> 00:34:54.429
It doesn't do anything.

00:34:54.429 --> 00:34:57.357
And the idea is you buy this
and you give it a purpose.

00:34:57.357 --> 00:34:59.190
You make it do something--
let's say produce

00:34:59.190 --> 00:35:01.700
a pharmaceutical or a biofuel.

00:35:01.700 --> 00:35:07.950
I've seen prototype products
that they take an E. coli

00:35:07.950 --> 00:35:13.290
bacteria that, let's say,
consumes sunlight and sugar

00:35:13.290 --> 00:35:16.410
and produces something
that feeds another E. coli

00:35:16.410 --> 00:35:19.440
bacteria, which produces a
biofuel, as sort of a step

00:35:19.440 --> 00:35:20.280
by step.

00:35:20.280 --> 00:35:23.940
Normally they wouldn't feed into
each other's processing loops,

00:35:23.940 --> 00:35:26.400
but somebody has
modified them so they do.

00:35:26.400 --> 00:35:28.770
Another very interesting
project that I saw, which you

00:35:28.770 --> 00:35:29.976
can see a TED Talk on this--

00:35:29.976 --> 00:35:32.100
of course, I don't remember
off the top of my head,

00:35:32.100 --> 00:35:33.960
but you could Google it--

00:35:33.960 --> 00:35:37.890
it is an art project where
they programmed organisms

00:35:37.890 --> 00:35:40.730
to grow chitin, that is
the shell that mollusks

00:35:40.730 --> 00:35:42.050
and others have.

00:35:42.050 --> 00:35:47.580
And the idea is we could
conceivably grow car fenders.

00:35:47.580 --> 00:35:49.710
So instead of having a
factory that produces,

00:35:49.710 --> 00:35:52.860
in very carbon-intensive
fashion, fenders,

00:35:52.860 --> 00:35:55.590
you could produce biodegradable
fenders that are pre-painted,

00:35:55.590 --> 00:35:57.690
because, you know, a
lobster has a color,

00:35:57.690 --> 00:35:59.580
but we could just give
it a different color.

00:35:59.580 --> 00:36:02.370
But all of those skills,
as you point out,

00:36:02.370 --> 00:36:04.710
and the wide availability of
E. coli bacteria, the fact

00:36:04.710 --> 00:36:09.600
that you can just order it from
online does make it very risky,

00:36:09.600 --> 00:36:12.900
that we might see people
designing custom organisms that

00:36:12.900 --> 00:36:14.730
could do great harm.

00:36:14.730 --> 00:36:16.200
And the only thing
I can tell you

00:36:16.200 --> 00:36:21.120
is, that's why you definitely
want society to be equitable,

00:36:21.120 --> 00:36:23.040
because the more
unjust a society is,

00:36:23.040 --> 00:36:25.350
I think the more likely
that's going to happen.

00:36:25.350 --> 00:36:28.170
It, to me, shows just how
rare that type of activity

00:36:28.170 --> 00:36:31.020
is, the fact that it isn't
constantly happening.

00:36:31.020 --> 00:36:34.380
It does happen, but
it's not widely common.

00:36:34.380 --> 00:36:37.590
And given how common all
of these components are

00:36:37.590 --> 00:36:41.020
and how relatively easy it
is to learn how to use them,

00:36:41.020 --> 00:36:43.660
it's actually miraculous
that it doesn't happen.

00:36:43.660 --> 00:36:45.420
I do think we'll start
to see more of it.

00:36:45.420 --> 00:36:48.030
But it's, if anything,
a good incentive for us

00:36:48.030 --> 00:36:51.310
to have a more just
society as a whole.

00:36:51.310 --> 00:36:51.810
Yes.

00:36:51.810 --> 00:36:52.351
AUDIENCE: Hi.

00:36:52.351 --> 00:36:55.680
So I should say that I'm
very impressed by the level

00:36:55.680 --> 00:36:57.911
of research you've
done for this book.

00:36:58.165 --> 00:36:59.040
DANIEL SUAREZ: Thanks

00:36:59.040 --> 00:37:02.470
AUDIENCE: That's not
usually the case.

00:37:02.470 --> 00:37:05.910
So I want to say
one thing that--

00:37:05.910 --> 00:37:09.240
I disagree with one kind of
assumptions you're making here.

00:37:09.240 --> 00:37:10.470
DANIEL SUAREZ: Let's hear it.

00:37:10.470 --> 00:37:13.650
AUDIENCE: So I think
that the general notion

00:37:13.650 --> 00:37:17.880
of genetic editing or genetic
engineering more broadly,

00:37:17.880 --> 00:37:20.660
biological engineering are
considered extremely dangerous,

00:37:20.660 --> 00:37:24.630
and no matter how
much we discuss it,

00:37:24.630 --> 00:37:28.080
there is an inherent
complexity here that

00:37:28.080 --> 00:37:30.570
is usually being overlooked.

00:37:30.570 --> 00:37:32.040
And it's kind of
what I'm saying is

00:37:32.040 --> 00:37:33.870
that it's inherently
complex in a way

00:37:33.870 --> 00:37:36.420
that it might be
impossible to compute, or--

00:37:36.420 --> 00:37:39.500
DANIEL SUAREZ: Like unintended
consequences are very likely.

00:37:39.500 --> 00:37:41.234
AUDIENCE: And I can use--

00:37:41.234 --> 00:37:42.525
so I'm a physicist by training.

00:37:42.525 --> 00:37:45.450
I can use various
different biology, physics,

00:37:45.450 --> 00:37:49.450
and computer science
descriptions of the problem.

00:37:49.450 --> 00:37:53.280
First, biologically,
in the 4 billion years

00:37:53.280 --> 00:37:56.400
of evolutionary
biology, like from

00:37:56.400 --> 00:38:00.600
the first photosynthetic system
to right now what we are,

00:38:00.600 --> 00:38:03.600
the system has always evolved
in a close equilibrium.

00:38:03.600 --> 00:38:08.490
So changes are very slow,
and entire ecosystem

00:38:08.490 --> 00:38:10.410
has time to respond.

00:38:10.410 --> 00:38:13.692
So because we are talking
about a complex system--

00:38:13.692 --> 00:38:15.150
DANIEL SUAREZ: And
death is usually

00:38:15.150 --> 00:38:17.460
involved in that process,
Darwinian selection.

00:38:17.460 --> 00:38:18.360
AUDIENCE: Exactly.

00:38:18.360 --> 00:38:22.410
But since the Homo
sapiens, potentially

00:38:22.410 --> 00:38:25.950
like a kind of a big change
in cognitive ability,

00:38:25.950 --> 00:38:30.000
80,000 years ago,
even in the Stone Age

00:38:30.000 --> 00:38:34.020
we already destroyed, before
even agricultural revolution,

00:38:34.020 --> 00:38:35.377
we destroyed half of mammals.

00:38:35.377 --> 00:38:36.210
DANIEL SUAREZ: Yeah.

00:38:36.210 --> 00:38:38.430
All the large
herbivores disappeared.

00:38:38.430 --> 00:38:42.910
AUDIENCE: Large mammals,
half of them were destroyed.

00:38:42.910 --> 00:38:46.210
And during the
agricultural revolution,

00:38:46.210 --> 00:38:48.560
Industrial Revolution,
we are destroying,

00:38:48.560 --> 00:38:52.496
extincting, on a daily basis,
on the order of thousands.

00:38:52.496 --> 00:38:53.870
DANIEL SUAREZ:
40% of species are

00:38:53.870 --> 00:38:56.630
supposed to be extinct
at this rate by 2050.

00:38:56.630 --> 00:38:57.230
40%.

00:38:57.230 --> 00:38:59.390
AUDIENCE: Exactly, yeah.

00:38:59.390 --> 00:39:03.530
But once you introduce this
kind of genetic engineering

00:39:03.530 --> 00:39:06.350
or editing, and including
various different kinds

00:39:06.350 --> 00:39:10.070
of other industries--

00:39:10.070 --> 00:39:14.120
nanotechnology, AI, this
combination of things

00:39:14.120 --> 00:39:17.270
drives the system such
out of equilibrium

00:39:17.270 --> 00:39:22.820
that there is no response time
to any of the biological--

00:39:22.820 --> 00:39:25.230
the natural biological
things to respond.

00:39:25.230 --> 00:39:28.920
And the system-- to downplay
this, like some people,

00:39:28.920 --> 00:39:31.530
policymakers can sit in a room
and figure this out-- this

00:39:31.530 --> 00:39:32.480
is very naive.

00:39:32.480 --> 00:39:34.100
DANIEL SUAREZ: You
know what this is?

00:39:34.100 --> 00:39:38.030
To me-- I'll absolutely
let you continue.

00:39:38.030 --> 00:39:39.587
It's not even a choice.

00:39:39.587 --> 00:39:40.670
This is already happening.

00:39:40.670 --> 00:39:43.700
When we say to allow
this to happen--

00:39:43.700 --> 00:39:45.620
again, these tools are
so readily available.

00:39:45.620 --> 00:39:47.900
What you're doing right
now is exactly what

00:39:47.900 --> 00:39:49.550
I would love to have happen--

00:39:49.550 --> 00:39:51.564
that is, people
calling for caution,

00:39:51.564 --> 00:39:53.480
and maybe taking action,
because the truth is,

00:39:53.480 --> 00:39:55.550
these tools are so
readily available.

00:39:55.550 --> 00:39:57.327
Nothing short-- I
mean, failing that--

00:39:57.327 --> 00:39:58.910
and there will be
corners of the world

00:39:58.910 --> 00:40:00.650
where this happens anyway.

00:40:00.650 --> 00:40:01.880
So please continue.

00:40:01.880 --> 00:40:03.260
AUDIENCE: So the
last statement I

00:40:03.260 --> 00:40:05.259
want to make that's
connecting with the audience

00:40:05.259 --> 00:40:09.110
here is that the solutions
to these kind of problems,

00:40:09.110 --> 00:40:14.010
predicting or trying to even
project any kind of outcome,

00:40:14.010 --> 00:40:15.830
would be near
impossible, because as

00:40:15.830 --> 00:40:17.155
far as peace and not peace.

00:40:17.155 --> 00:40:18.780
So it's like these
are wars, because we

00:40:18.780 --> 00:40:22.244
are approaching wars,
[INAUDIBLE] hard problems.

00:40:22.244 --> 00:40:23.660
What I'm saying
is that people who

00:40:23.660 --> 00:40:27.470
think that they can predict
this thing are extremely naive.

00:40:27.470 --> 00:40:32.630
So what I'm saying is that there
might be serious consequences,

00:40:32.630 --> 00:40:35.000
unintended-- forget about
intended consequences--

00:40:35.000 --> 00:40:38.770
unintended consequences, because
the combinatory possibilities

00:40:38.770 --> 00:40:41.035
is just beyond
evaluation, basically.

00:40:41.035 --> 00:40:42.410
DANIEL SUAREZ:
The reason I'm not

00:40:42.410 --> 00:40:44.450
disagreeing with you
is that is basically

00:40:44.450 --> 00:40:49.020
most of what my book's about, is
those unintended consequences.

00:40:49.020 --> 00:40:50.780
And when I stand
here and say that I'm

00:40:50.780 --> 00:40:52.613
optimistic about the
future, the only reason

00:40:52.613 --> 00:40:54.650
I say that is we
have the ability

00:40:54.650 --> 00:40:57.800
to annihilate the human
race, all creatures on Earth,

00:40:57.800 --> 00:41:00.020
and we've had that ability
for half a century,

00:41:00.020 --> 00:41:01.880
and we haven't done it yet.

00:41:01.880 --> 00:41:03.650
And I'm with you.

00:41:03.650 --> 00:41:04.190
I hear you.

00:41:04.190 --> 00:41:06.065
There's a tremendous
amount of hubris

00:41:06.065 --> 00:41:08.690
that might come into play here,
where we think we can play God,

00:41:08.690 --> 00:41:12.050
and we're just still
toddlers at it.

00:41:12.050 --> 00:41:14.450
But at the same time, I know
the tools are out there,

00:41:14.450 --> 00:41:17.360
and a lot of these
changes, rapid editing,

00:41:17.360 --> 00:41:19.230
are going to occur.

00:41:19.230 --> 00:41:21.530
So whether that's people
using supercomputing

00:41:21.530 --> 00:41:25.220
to do bioinformatics to
try to understand genetics

00:41:25.220 --> 00:41:26.870
and how all of these
things connect,

00:41:26.870 --> 00:41:29.390
I think it is going to
be a race against time.

00:41:29.390 --> 00:41:33.200
And your concern, actually,
is, I think, well founded.

00:41:33.200 --> 00:41:36.240
But I do think this is going
to start to happen regardless.

00:41:36.240 --> 00:41:39.170
And so I would love everybody
in this room to get involved

00:41:39.170 --> 00:41:41.820
and to pay attention
to what's going on.

00:41:41.820 --> 00:41:44.150
And you'll see just
how concerned I am--

00:41:44.150 --> 00:41:45.860
not to force you
to read the book,

00:41:45.860 --> 00:41:47.760
but there's a great
deal in there--

00:41:47.760 --> 00:41:50.150
this is why I'm sometimes
called a doomsayer.

00:41:50.150 --> 00:41:53.300
People say, oh, come on,
Dan, you're ruining the fun.

00:41:53.300 --> 00:41:55.150
I like to think what
I do in my books--

00:41:55.150 --> 00:41:56.900
this is why they're
thrillers, by the way,

00:41:56.900 --> 00:42:00.320
is because there's typically
a perilous outcome at stake.

00:42:00.320 --> 00:42:03.410
I like to think what
I do is spot icebergs.

00:42:03.410 --> 00:42:06.980
And it's much better to
prototype the future in fiction

00:42:06.980 --> 00:42:09.080
than it is to run into it blind.

00:42:09.080 --> 00:42:11.122
And I'm not saying
let's all abandon ship.

00:42:11.122 --> 00:42:13.580
I'm saying, let's turn a little
to the right, turn a little

00:42:13.580 --> 00:42:14.670
to the left.

00:42:14.670 --> 00:42:16.790
And that's basically
what I try to do.

00:42:16.790 --> 00:42:18.720
So I agree with you.

00:42:18.720 --> 00:42:20.012
I think you're dead on.

00:42:20.012 --> 00:42:21.470
And I guess we'll
see what happens.

00:42:24.600 --> 00:42:28.730
AUDIENCE: So my question is more
along the lines of writing--

00:42:28.730 --> 00:42:30.360
the writing side of things.

00:42:30.360 --> 00:42:31.600
So one of the reasons--

00:42:31.600 --> 00:42:34.820
I came to your last
talk here as well,

00:42:34.820 --> 00:42:37.130
and one of the reasons that
I really like your books

00:42:37.130 --> 00:42:39.470
and like these talks
is that you're really

00:42:39.470 --> 00:42:42.160
obviously very interested
in all of this stuff.

00:42:42.160 --> 00:42:43.160
DANIEL SUAREZ: Oh, I am.

00:42:43.160 --> 00:42:44.750
AUDIENCE: Like really
intellectually--

00:42:44.750 --> 00:42:46.958
you can tell when you're up
there talking about this,

00:42:46.958 --> 00:42:48.680
you're like, this
is really cool.

00:42:48.680 --> 00:42:49.777
[LAUGHTER]

00:42:49.777 --> 00:42:51.110
DANIEL SUAREZ: Well, yeah, I do.

00:42:51.110 --> 00:42:53.420
AUDIENCE: Yeah.

00:42:53.420 --> 00:42:57.650
So I'm kind of
curious, obviously

00:42:57.650 --> 00:43:01.820
you've bounced around to a
lot of very different, very

00:43:01.820 --> 00:43:04.580
interesting topics
in their own way,

00:43:04.580 --> 00:43:09.467
and run the genetic
editing side of things.

00:43:09.467 --> 00:43:11.300
Obviously at some point
you'll be picking up

00:43:11.300 --> 00:43:13.910
something for your next book.

00:43:13.910 --> 00:43:16.214
When does that
transition for you?

00:43:16.214 --> 00:43:16.880
Because you're--

00:43:16.880 --> 00:43:17.870
DANIEL SUAREZ: It's
already occurred.

00:43:17.870 --> 00:43:19.130
I'm already working
on my next book.

00:43:19.130 --> 00:43:20.300
AUDIENCE: So you're
already moving on.

00:43:20.300 --> 00:43:21.920
DANIEL SUAREZ: That's why I have
to be careful not to mention

00:43:21.920 --> 00:43:23.503
things that don't
happen in this book,

00:43:23.503 --> 00:43:25.700
but you know, that's
the way it is.

00:43:25.700 --> 00:43:28.565
There's a saying that
you have your whole life

00:43:28.565 --> 00:43:30.440
to write your first
book, and you have a year

00:43:30.440 --> 00:43:31.854
to write your second one.

00:43:31.854 --> 00:43:33.770
It's typically because
if you've done it right

00:43:33.770 --> 00:43:34.640
and you've managed
to break through,

00:43:34.640 --> 00:43:36.530
they're like, oh,
great, do another one.

00:43:36.530 --> 00:43:39.170
And I've been fortunate
with my publisher, Dutton,

00:43:39.170 --> 00:43:41.510
part of Penguin Random
House, that I've just

00:43:41.510 --> 00:43:43.240
been able to really
focus on the writing.

00:43:43.240 --> 00:43:45.130
And they've been always there
to back me up and give me

00:43:45.130 --> 00:43:47.470
a new book deal, and I
always have something else

00:43:47.470 --> 00:43:48.700
that I'm interested in.

00:43:48.700 --> 00:43:51.034
A lot of people have asked
me, like with "Daemon"

00:43:51.034 --> 00:43:52.450
and "FreedomTM"
were the first two

00:43:52.450 --> 00:43:54.840
books that I did
that focused on cyber

00:43:54.840 --> 00:43:56.140
security and other issues--

00:43:56.140 --> 00:43:58.424
really a lot more than that--

00:43:58.424 --> 00:44:00.340
whether I would do another
one of those books.

00:44:00.340 --> 00:44:03.280
And what I typically
say is, I may,

00:44:03.280 --> 00:44:05.710
but I have such
wide-ranging interests

00:44:05.710 --> 00:44:09.179
that I would hate to just stay
writing the same books again

00:44:09.179 --> 00:44:10.720
and again, because
I don't want to go

00:44:10.720 --> 00:44:12.040
too many times to the well.

00:44:12.040 --> 00:44:15.110
But also, I want to keep
my fascination fresh.

00:44:15.110 --> 00:44:18.160
And there is just so many
interesting, risky, cool,

00:44:18.160 --> 00:44:21.580
and provocative things going
on with technology in the world

00:44:21.580 --> 00:44:25.780
and how it impacts society
that it this becomes a dream

00:44:25.780 --> 00:44:28.240
job for me, to be able to
go around and try to think

00:44:28.240 --> 00:44:31.330
about what I want to write,
and then have the opportunity

00:44:31.330 --> 00:44:32.860
to do it is great.

00:44:32.860 --> 00:44:36.820
So I never run out of
stuff, so far, to write.

00:44:36.820 --> 00:44:39.730
And that's why I tend to write
different books every time,

00:44:39.730 --> 00:44:41.760
as opposed to another sequel.

00:44:41.760 --> 00:44:43.740
So I don't know. hopefully--

00:44:43.740 --> 00:44:45.140
AUDIENCE: Can you give us a
hint of what's coming next?

00:44:45.140 --> 00:44:45.750
DANIEL SUAREZ: What's that?

00:44:45.750 --> 00:44:47.060
AUDIENCE: Can you give us a
hint of what's coming next?

00:44:47.060 --> 00:44:49.435
DANIEL SUAREZ: Well, actually
I can tell you why I won't.

00:44:49.435 --> 00:44:51.110
[LAUGHTER]

00:44:51.110 --> 00:44:52.639
The world has changed a bit.

00:44:52.639 --> 00:44:54.180
There's a couple
reasons why I don't.

00:44:54.180 --> 00:44:57.410
I think the first reason
I don't is I found--

00:44:57.410 --> 00:44:59.780
and maybe you find
this, too, that when

00:44:59.780 --> 00:45:01.160
you want to write
a story and you

00:45:01.160 --> 00:45:03.125
start telling your
friends about it,

00:45:03.125 --> 00:45:05.750
that you find that you take more
and more time talking about it

00:45:05.750 --> 00:45:07.120
than actually doing it.

00:45:07.120 --> 00:45:08.953
And this was very much
the case for "Daemon"

00:45:08.953 --> 00:45:11.570
when I wrote it, when I was
not a professional writer.

00:45:11.570 --> 00:45:14.030
I could spend some of my
free time talking about it,

00:45:14.030 --> 00:45:15.530
or I could do it.

00:45:15.530 --> 00:45:18.920
And I found if I prohibited
myself from talking about it

00:45:18.920 --> 00:45:20.510
and instead wrote
it, and I can only

00:45:20.510 --> 00:45:23.060
talk about it when it was
done, that that worked out

00:45:23.060 --> 00:45:24.260
much better for me.

00:45:24.260 --> 00:45:26.660
There was also a
commercial aspect of it,

00:45:26.660 --> 00:45:29.900
which is if I mentioned
what I'm writing,

00:45:29.900 --> 00:45:33.440
there might be a super short
novella up on Amazon three days

00:45:33.440 --> 00:45:35.390
from now about that.

00:45:35.390 --> 00:45:37.830
So that's just the reality
of the modern world.

00:45:37.830 --> 00:45:42.110
And I really do like to try
to preserve the originality

00:45:42.110 --> 00:45:44.420
or the impact of
what I'm writing.

00:45:44.420 --> 00:45:45.980
So those two things
work together

00:45:45.980 --> 00:45:48.530
for me to say, no comment.

00:45:48.530 --> 00:45:50.900
[LAUGHTER]

00:45:50.900 --> 00:45:51.630
Anybody else?

00:45:51.630 --> 00:45:53.740
Any questions at all?

00:45:53.740 --> 00:45:56.560
Good questions, man.

00:45:56.560 --> 00:45:57.690
All right, I think--

00:45:57.690 --> 00:45:59.658
oh, yes.

00:45:59.658 --> 00:46:02.340
AUDIENCE: Most of
your books are based

00:46:02.340 --> 00:46:06.380
on the idea that doesn't
allow you to sleep at night.

00:46:06.380 --> 00:46:12.575
What are a couple ideas that
[INAUDIBLE] discarded that--

00:46:12.575 --> 00:46:14.200
DANIEL SUAREZ: That's
a great question.

00:46:14.200 --> 00:46:16.120
Since you weren't
at the microphone,

00:46:16.120 --> 00:46:21.689
the question was if what I write
about comes from ideas that

00:46:21.689 --> 00:46:23.230
won't let me go,
let's say-- it's not

00:46:23.230 --> 00:46:24.605
that I won't sleep
at night, it's

00:46:24.605 --> 00:46:26.950
that it's typically
when my mind is

00:46:26.950 --> 00:46:29.650
in its passive, resting
state, my mind immediately

00:46:29.650 --> 00:46:32.080
goes back to that thing.

00:46:32.080 --> 00:46:35.200
What are some examples of ideas
that I did not write about?

00:46:38.410 --> 00:46:43.490
This goes into my Google search
history and security watch

00:46:43.490 --> 00:46:44.387
list.

00:46:44.387 --> 00:46:45.670
[LAUGHTER]

00:46:45.670 --> 00:46:47.830
I, for a while, had some
difficulty getting back

00:46:47.830 --> 00:46:49.690
into the country without
interested people

00:46:49.690 --> 00:46:50.900
wanting to talk to me.

00:46:50.900 --> 00:46:52.540
And this has been
sorted out since.

00:46:52.540 --> 00:46:55.405
I have a very alarming
Google search history.

00:46:55.405 --> 00:46:57.250
A typical day for
me would involve

00:46:57.250 --> 00:47:01.960
shaped charges, sniper rifles,
bio weapons, just on and on.

00:47:01.960 --> 00:47:09.320
And a lot of ideas that involve
innovative ways to cause harm

00:47:09.320 --> 00:47:10.720
are ideas that tend to go away.

00:47:10.720 --> 00:47:13.100
They might fascinate
me for a while,

00:47:13.100 --> 00:47:15.880
but I don't want to be too
instructive, because I think

00:47:15.880 --> 00:47:19.570
my problem is I was a D and D
moderator for about 10 years,

00:47:19.570 --> 00:47:22.690
and I can make traps
that are very devious.

00:47:22.690 --> 00:47:24.880
So I don't want to go too
deep into that territory.

00:47:24.880 --> 00:47:27.070
Those are typically the
ideas that I abandon,

00:47:27.070 --> 00:47:28.750
looking at that
saying, oh my God,

00:47:28.750 --> 00:47:32.320
how awful I'll feel if some
person says, yes, that's it,

00:47:32.320 --> 00:47:34.630
and they run out and
start to do that.

00:47:34.630 --> 00:47:35.870
That would be bad.

00:47:35.870 --> 00:47:39.070
So that's my answer.

00:47:39.070 --> 00:47:39.700
Yes.

00:47:39.700 --> 00:47:41.514
AUDIENCE: Two words--
incognito mode.

00:47:41.514 --> 00:47:43.930
DANIEL SUAREZ: Oh, yeah, I
spend a lot of time doing that.

00:47:43.930 --> 00:47:45.430
But I think I'm beyond that.

00:47:45.430 --> 00:47:47.050
Also, there's proxies.

00:47:47.050 --> 00:47:49.219
There's Tor and on and on.

00:47:49.219 --> 00:47:51.010
But see, now I have to
worry less about it,

00:47:51.010 --> 00:47:53.051
because I think they know
me, and it's like, oh--

00:47:53.051 --> 00:47:54.520
oh no, it's just Dan.

00:47:54.520 --> 00:47:55.020
It's fine.

00:47:55.020 --> 00:47:56.080
It's not a terrorist.

00:47:56.080 --> 00:47:58.870
Yes.

00:47:58.870 --> 00:48:03.610
AUDIENCE: What books
are you reading now?

00:48:03.610 --> 00:48:06.080
DANIEL SUAREZ: OK, this
ought to alarm everybody.

00:48:06.080 --> 00:48:09.566
I'm about halfway through
"The Sixth Extinction."

00:48:09.566 --> 00:48:10.690
It's very cheerful reading.

00:48:10.690 --> 00:48:12.314
But that's why I knew
that about half--

00:48:12.314 --> 00:48:14.450
or 40% of the
species in the world

00:48:14.450 --> 00:48:17.020
were going to be wiped
out by 2050 if we

00:48:17.020 --> 00:48:19.000
don't change what we're doing.

00:48:19.000 --> 00:48:20.942
What other books have I read?

00:48:20.942 --> 00:48:22.900
"All the Light We Cannot
See," I read recently.

00:48:22.900 --> 00:48:24.108
That's such a beautiful book.

00:48:24.108 --> 00:48:26.740
I sometimes like to read
real literary fiction, which

00:48:26.740 --> 00:48:28.640
is very different
from what I do,

00:48:28.640 --> 00:48:30.610
where each sentence
and paragraph is

00:48:30.610 --> 00:48:32.290
very beautiful and evocative.

00:48:32.290 --> 00:48:34.150
I will admit that's
not what I do.

00:48:34.150 --> 00:48:36.140
My books tend to be about ideas.

00:48:36.140 --> 00:48:37.660
I also find it
much more relaxing

00:48:37.660 --> 00:48:40.930
to read books that are in
a genre that I don't write,

00:48:40.930 --> 00:48:43.630
because otherwise, if I
read another tech thriller,

00:48:43.630 --> 00:48:46.000
I tend to look for the
scaffolding all the time,

00:48:46.000 --> 00:48:48.250
or say, oh, no, no, I
would have put this here.

00:48:48.250 --> 00:48:50.590
And it takes you out of
the story all the time.

00:48:50.590 --> 00:48:53.096
So-- also "Name of the
Wind," I read recently,

00:48:53.096 --> 00:48:54.720
Patrick Rothfuss,
which I really liked.

00:48:54.720 --> 00:48:59.720
Again, fantasy-- I could just
kind of relax and enjoy it.

00:48:59.720 --> 00:49:00.650
Yes.

00:49:00.650 --> 00:49:03.020
AUDIENCE: So as people
begin body hacking

00:49:03.020 --> 00:49:04.700
and modifying
themselves with genes,

00:49:04.700 --> 00:49:07.550
do you feel that we might
become more diverse or more

00:49:07.550 --> 00:49:10.509
homogeneous as people start
just altering themselves?

00:49:10.509 --> 00:49:12.300
DANIEL SUAREZ: Oh,
that's a great question.

00:49:12.300 --> 00:49:13.758
So again, the
question was, will we

00:49:13.758 --> 00:49:16.280
become more diverse or more
homogeneous as a result?

00:49:16.280 --> 00:49:19.670
Well, I think the web is super
instructive with this regard.

00:49:19.670 --> 00:49:21.620
If you remember the
early days of the web,

00:49:21.620 --> 00:49:24.320
there was this profusion, this--

00:49:24.320 --> 00:49:27.090
I don't know, websites all over.

00:49:27.090 --> 00:49:29.120
And then what happened
was these certain portals

00:49:29.120 --> 00:49:32.150
started to develop,
and one comes to mind.

00:49:32.150 --> 00:49:35.820
But it's interesting
how initially, there

00:49:35.820 --> 00:49:37.190
was this great profusion.

00:49:37.190 --> 00:49:39.390
But then almost a
culture builds around it,

00:49:39.390 --> 00:49:42.620
and people start to create
a vocabulary and a language,

00:49:42.620 --> 00:49:46.730
and they start to aggregate
towards common themes.

00:49:46.730 --> 00:49:49.400
And I think if we have the
ability to genetically edit

00:49:49.400 --> 00:49:54.770
ourselves, I don't know whether
it will become Second Life,

00:49:54.770 --> 00:49:57.650
or whether people will say,
oh, I really like that--

00:49:57.650 --> 00:49:59.240
like fashion, like clothing.

00:49:59.240 --> 00:50:02.360
I mean, I think clothing is
probably a perfect example.

00:50:02.360 --> 00:50:04.930
We don't-- we wear
different enough things,

00:50:04.930 --> 00:50:08.750
but I didn't come here in a suit
of armor or as a Shakespearean

00:50:08.750 --> 00:50:09.520
character.

00:50:09.520 --> 00:50:11.228
And I don't think the
trend is for people

00:50:11.228 --> 00:50:12.670
to be that different.

00:50:12.670 --> 00:50:13.940
Now, that's my instinct on it.

00:50:13.940 --> 00:50:14.810
I might be wrong.

00:50:14.810 --> 00:50:18.310
But I think people will see
the reaction other people get.

00:50:18.310 --> 00:50:20.480
I think tastes and
mores might affect it.

00:50:20.480 --> 00:50:23.960
So I think initially, we'll
see a great profusion as people

00:50:23.960 --> 00:50:26.240
try things out,
but then concerns

00:50:26.240 --> 00:50:28.700
with health, unintended
consequences,

00:50:28.700 --> 00:50:32.360
and other things will cause
people to do sort of the mall

00:50:32.360 --> 00:50:34.360
version of a lizard person.

00:50:34.360 --> 00:50:36.740
It's like, well,
it's just the skin.

00:50:36.740 --> 00:50:39.560
I'm still the same underneath.

00:50:39.560 --> 00:50:41.210
So that's probably
the weirdest answer

00:50:41.210 --> 00:50:44.990
to that question you were
thinking you could get.

00:50:44.990 --> 00:50:46.610
All right, anybody else?

00:50:50.274 --> 00:50:52.940
I'm having fun, so anybody?

00:50:52.940 --> 00:50:54.330
All right.

00:50:54.330 --> 00:50:55.360
I think [INAUDIBLE].

00:50:55.360 --> 00:50:56.260
[APPLAUSE]

00:50:56.260 --> 00:50:56.860
Thank you.

00:50:56.860 --> 00:50:58.710
Thank you.
