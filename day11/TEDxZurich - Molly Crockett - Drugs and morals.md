https://www.youtube.com/watch?v=1_-2wOnkc_k&ab_channel=TEDxTalks 


TEDxZurich - Molly Crockett - Drugs and morals 


WEBVTT Kind: captions Language: en 

Transcriber: Mohand Habchi Reviewer: Brenda Oreggioni 

Thank you so much. I am going to start with a very deep philosophical question. How do you feel about blue cheese? You know `stilton`, `roquefort`, `gorgonzola`. Most people have pretty strong opinions, where one or the other, they love it or hate it, I'd like you to imagine someone who does not share your opinion on blue cheese, and consider whether you'd be happy to have this person, as a neighbor, a close friend, or a romantic partner? And keeps these feelings in mind. Another controversial topic is abortion. Should it be legal or illegal? Think about your own opinion and now consider whether you'd be happy to have someone who doesn't share your opinion on abortion, as a neighbor, a close friend, or a romantic partner? Not quite the same as feelings on `Stilton`, right? These questions I've asked you are adapted from research by Linda Skitka at the University of Illinois. Her research has shown that there's something special about `moral` values, like opposition to abortion, as opposed to non-moral values, like your feelings about blue cheese. And as you might expect, disagreement on moral issues is much more damaging to social relationships than disagreement on non-moral issues. Why is it that a blue cheese lover is perfectly happy to be friend, marry, have kids with a blue cheese hater, but there exist anti-abortion extremists, who think it's `justifiable` to kill another person in a Church, just because he disagrees with them. We've got to figure this out because we now live in a world where `extremists` guided by their moral `convictions` can do a lot of damages. We can start by asking ourselves, "How it is that we know something is right or wrong?" And it turns out actually for a lot people this question doesn't even really make any sense, because people often experience moral believes as if they're objective facts about the world. So for someone oppose to the death penalty for example, the fact that it's wrong seems as obvious as the statement 1+1=2. And we have some more evidence for this from research by Geoffrey Goodwin at the University of Pennsylvania. He runs some experiments were he presented his volunteers with a series of statements in the following categories: Facts, like Boston is further north than Los Angeles, Ethics, like opening gunfire on a crowded city street is wrong. Social norms, like wearing pajamas to a TED talk is wrong. Tastes, like classical music is better than rock music. For each statement, subject had to answer yes or no to the following questions. Does the statement have a correct answer? And here's what they found. Not surprisingly, people felt most strongly that facts had correct answers while tastes did not. But notice that the statement of ethics looked more like facts than like tastes. And we see this overlap between facts and values in the brain as well. Sam Harris and colleagues scanned people's brains as they evaluated the truthfulness of factual statements, ethical statements, and religious statements. They found that a region of the brain called the medial prefrontal cortex, was more active when people believed the statement to be true than to be false. But importantly activity in this region did not differentiate between the different categories of moral beliefs of different statements. So mathematical beliefs, like 2+2=4, show the similar pattern of activity to ethical beliefs. Like, "It's wrong to take pleasure at another's suffering." Now the upshot of all this, is we think that there is a right answer to moral questions. And `here's the rub.` If you and I disagree, and we both can be right, well, who's right? And of course the answer is obvious, it's me who's right. Obviously my facts trump your facts. And therefore you must be stupid or unreasonable. And of course this kind of language is all too common in politics these days. (Laughter) But there's also a dangerous difference  between disagreeing on facts and disagreeing on moral issues. See if you think that 1+1=3, I might think you're unreasonable or a little strange. But if you and I disagree on a moral issue, not only do I think that you're unreasonable, but also a bad person. Maybe even less than human. Moral values are like facts on `steroids`. They've got really strong emotion attach to them, and these emotions often come with a motivation to harm or eliminate the other side. And this is of course is a huge problem. Because while we readily accept that tastes and opinions can change, facts are facts. I have my facts, you have your facts, and we're both so committed to those realities that it's senseless to expect that either of us will ever change. Imagine trying to convince someone who is red-green color blind, that these two circles are different colors. There is nothing you can say it to this person to make him see the world the way you see it. And the same unfortunately appears to be true with differences in moral viewpoints. Values seem like facts, and facts by definition are fixed properties of reality. So where do we go from here? I grew up in the States, but I've been in Europe for about five years now, and I watched as the political situation has become increasingly `insane`, increasingly `extreme`, and I really wanted to understand how it is that we get so `attached` to our moral values. Myself included. And I'm a neuroscientist, so naturally I started poking around in people's brains. And I found out that our moral values are lot less stable than we think. What if I told you that a pill could change your judgment of what's right and what's wrong. Or what if I told you that your sense of justice could depend on what you had for breakfast this morning. You're probably thinking by now that this sounds like science-fiction. Neurons in the brain use chemicals called neurotransmitters to talk to each other. Here we have two neurons, the gap between them is called the synapse. To transmit a message across the `synapse`, this one neuron must release neurotransmitters into the synapse where they bind to receptors on the next neuron and `propagate` the message. The brain produces and releases these chemicals in response to various situations. My colleagues and I wanted to know  whether manipulating people's brain chemistry could change the way they respond to moral situations. So in one study, we presented our volunteers with eight moral `dilemmas`, like the following. There's a trolley and it's headed out of control down some tracks towards five workers who will die if you do nothing. However you can stop the trolley by pushing a man carrying a heavy briefcase in front of the trolley onto the tracks, and he will die but the five other will be saved. The question is? Is it morally acceptable to harm this one person to save the others. And of course there's no objectively correct answer to this question. And in fact, there are two schools of moral thought that take opposing views. So the `utilitarian` school, rooted in the works of Jeremy Bentham, judges' actions based on the outcome they produce. So morally appropriate actions are those that result in the greatest good for the greatest numbers. On the other hand, the `deontological` school, grounded in the works of Immanuel Kant, judges the actions themselves. So there are right actions and wrong actions and the outcomes are irrelevant. In the dilemma I just described to you, utilitarians would say it's `appropriate` to push the man in front of the `trolley`, because more lives will be saved in the end, whereas deontologists would say it's `inappropriate`, because harming is fundamentally wrong. My colleagues and I asked 30 volunteers to make judgments in a series of moral dilemmas like the one I described to you. We wanted to see what if we could change people's judgments of right and wrong by manipulating a particular brain chemical called `serotonin`. We used a drug called the Selective Serotonin Reuptake Inhibitor, or SSRI. It's similar to the antidepressant, Prozac, and basically works by enhancing the effect of serotonin in the brain. On one session our volunteers made moral judgments while on the influence of SSRI. And on the other session they made moral judgments while on a placebo pill. And here's what we found. On the placebo session, our volunteers said it was appropriate to harm one to save many others in about 40% of the cases that we described to them. And when we gave them the SSRI, they were significantly less likely to say it was acceptable to harm one to save many. In other words, the drug made them less utilitarian. Now take a second to think about these results. The debate between utilitarians and `deontologists` has been raging for hundreds of years. We gave people a single pill and without even knowing it, they gave different answers to this question of whether or not it's OK to harm one to save many others. So could the difference between people like Bentham and people like Kant, boil down to the serotonin levels in their brains? And on a more serious note, what are the implications of this for other kinds of ethical questions? So taking this idea further, my colleagues also looked at whether serotonin levels influence the way that we respond to being treated unfairly. We used a game from economics called "The ultimatum game." There are two players, a `proposer` and a `responder`. The proposer suggests the way to split a sum of money with the responder. The responder can either accept the offer in which case both players are paid accordingly, or he can reject the offer in which case neither player gets any money. Many studies have now shown responders will typically reject offers they perceive to be unfair. Which makes sense. I think a lot of us would rather have nothing than let someone who's treated us unfairly get the lion's share. In our study, we wanted to see whether we could shift around people's responses to unfairness by changing their serotonin levels. We did this by changing people's diet. So, the raw ingredient for serotonin is the amino acid tryptophan. And we all must constantly replenish our supply of tryptophan by eating protein-rich foods. In the lab, we are able to manipulate serotonin levels, by giving people a protein shake that lacks tryptophan. And on the placebo control session we give people a shake that looks and tastes the same except that it does contain 2.5 grams of added tryptophan. And we gave people these drinks and had them play the `ultimatum` game in the role of responder, then we measured rejection rates for unfair, medium and fair offers. On placebo, you can see people reject a lot of the unfair offers, they hardly ever reject the fair 50/50 splits, and here's what happen when we lower their serotonin levels: Rejection rates go up for the unfair offers. Now take a second again to just pause and think about this. The only difference between these two treatments is 2.5 grams of tryptophan in the diet. That's it. Our volunteers don't feel any difference in the two sessions, they don't notice any changes in their behavior. And yet the `subtle` difference was enough to change the way that they responded to being treated unfairly. Now, it's important to `point out`, in this lab study we artificially manipulated people's serotonin levels, but out in the real world, serotonin levels do fluctuate naturally. In response to things like changes in diet and stress levels. What this means is our moral values could be shifting a little bit all the time without us even knowing it. And we do have evidence that this kind of thing is happening out in the real world. Shai Danziger and colleagues and I looked at parole board rulings, judges' rulings of whether to grant parole to prisoners. Here, we have the proportion of favorable rulings on the vertical axis, and on the bottom, we have basically the time of day. Here on the vertical axis are the judges meal breaks. It turns out if you're coming up for `parole`, you're more likely to be granted parole if your hearing takes place after the judge had a snack. I hope that this worries you a little bit. (Laughter) More seriously, I hope that I have convinced you that our moral values are less stable than they appear to be. And this is important, because it turns out that simply believing that `moral values are changeable as opposed to fixed` can have dramatic effects on our willingness to cooperate and compromise. The Israel-Palestine conflict, is one of the biggest `ideological` `clashes` of our time. It's resulted in thousands of deaths on both sides, huge cost in quality of life. Eran Halperin, Carol Dweck and colleagues recently reported that beliefs about whether groups have a fixed versus changeable nature can influence Israelian and Palestinian attitudes towards each other and their willingness to compromise for peace. In their experiment, Israelis and Palestinians were randomly assigned to read one of two articles. One article suggested that aggressive groups have a fixed nature, and the other article suggested they have a changeable nature. Those who read the article about changeable groups were more likely to be willing to meet with the other side and hear their point of view, and more willing to compromise on issues like the status of Jerusalem and settlements in the West Bank.. What this means is... If we can wrap our heads around the idea that moral values are not fixed but can change, we are more likely to listen to each other. And here's a crazy idea. If pills can shift our moral values, what if negotiators popped a few moral enhancers before heading to the table? (Laughter) Such an intervention might make it easier for opponents to see each other's side. Now of course we have a long way to go before we fully understand which brain chemicals influence which kind of moral beliefs. But I do think it's `plausible` that one day  we will have the `expertise` to identify different brain systems that drive preferences for conflicting ethical principles. As long as we believe, falsely, I might add, that our moral values are unshakeable, we will continue to invest our resources in fighting with each other rather than searching for a middle way. Instead, can't we `cultivate` a healthy sense of `skepticism` for our own sense of right and wrong? Because once we accept that our moral values can be altered by factors beyond our awareness and control, maybe we'll become a little less attached to them. And as sooner that we can let go of this `attachment`, the better. Because we've got some scary problems, threatening our collective survival. And we're not solving them because we're sitting around `bickering` with each other, fighting with each other, `drowning` in this quick sand of hate and fear. This hate and this fear are blinding us to our common `humanity` and the amazing things that we can achieve if we put our differences aside and our heads and hearts together. People, it's time to open our eyes, to open ourselves and to wake up. Thank you. (Applause) 